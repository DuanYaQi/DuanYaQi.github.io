<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="DL," />










<meta name="description" content="深度学习工程师由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。">
<meta property="og:type" content="article">
<meta property="og:title" content="C_DL_W2">
<meta property="og:url" content="http://duanyaqi.com/2022/03/28/C-DL-W2/index.html">
<meta property="og:site_name" content="Duan&#39;s Blog">
<meta property="og:description" content="深度学习工程师由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/7be8de26db579238f7e72a5a45087595.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/image-20210330210829773.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/image-20210330221922620.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617160374854.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617162083581.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617162158684.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617165180456.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617165195446.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167345080.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167554679.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167574641.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167588247.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167613864.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617167911663.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617245938449.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/equation.svg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/equation-1617340169208.svg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617370541220.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171026113219156">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171026135131370">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617372894803.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171027155944527">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171028095447301">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171028142838569">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/1617433470211.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171028163526337">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/v2-5db14a3057bc9c9c407ede98f14eb6f6_720w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/20171101094754376">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/2d314293ae9aaa67285299267857632f.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/equation-1617519745354.svg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/v2-11758fbc2fc5bbbc60106926625b3a4f_1440w.jpg">
<meta property="article:published_time" content="2022-03-28T07:44:53.000Z">
<meta property="article:modified_time" content="2022-03-28T07:53:43.680Z">
<meta property="article:author" content="Duan Yaqi">
<meta property="article:tag" content="DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://duanyaqi.com/2022/03/28/C-DL-W2/assets/7be8de26db579238f7e72a5a45087595.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://duanyaqi.com/2022/03/28/C-DL-W2/"/>





  <title>C_DL_W2 | Duan's Blog</title>
  








<meta name="generator" content="Hexo 6.0.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a target="_blank" rel="noopener" href="https://github.com/DuanYaQi" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Duan's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://duanyaqi.com/2022/03/28/C-DL-W2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Duan's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">C_DL_W2</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-28T15:44:53+08:00">
                2022-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  5.9k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  23
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习工程师"><a href="#深度学习工程师" class="headerlink" title="深度学习工程师"></a>深度学习工程师</h1><p>由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。<br><span id="more"></span></p>
<p>deeplearning.ai</p>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll">https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll</a></p>
<p><a target="_blank" rel="noopener" href="https://study.163.com/my#/smarts">https://study.163.com/my#/smarts</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66524657">https://www.bilibili.com/video/av66524657</a></p>
<p><strong>note</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/red_stone1/article/details/78403416">https://blog.csdn.net/red_stone1/article/details/78403416</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/DeepLearningNotebook">https://www.zhihu.com/column/DeepLearningNotebook</a></p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p>
<p><strong>课后作业</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013733326/article/details/79827273">https://blog.csdn.net/u013733326/article/details/79827273</a></p>
<p><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/5e20243e2823a10036b542da">https://www.heywhale.com/mw/project/5e20243e2823a10036b542da</a></p>
<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ul>
<li>[ ] 改善深层神经网络-<a href="#winit">1.11 权重初始化</a>，有这个概念，但没在主流模型的代码中见过。</li>
<li>[ ] 改善深层神经网络-<a href="#BN-test">3.7 测试时的 Batch Norm</a>，指数加权平均过程很模糊 </li>
<li>[ ] 改善深层神经网络-<a href="#dadz">3.9 训练一个 Softmax 分类器</a>，激活函数对 z 求导，<a target="_blank" rel="noopener" href="https://www.cnblogs.com/lizhiqing/p/10684795.html">https://www.cnblogs.com/lizhiqing/p/10684795.html</a></li>
</ul>
<hr>
<h2 id="改善深层神经网络：超参数调试、正则化以及优化"><a href="#改善深层神经网络：超参数调试、正则化以及优化" class="headerlink" title="改善深层神经网络：超参数调试、正则化以及优化"></a>改善深层神经网络：超参数调试、正则化以及优化</h2><h3 id="第一周-深度学习的实用层面"><a href="#第一周-深度学习的实用层面" class="headerlink" title="第一周 深度学习的实用层面"></a>第一周 深度学习的实用层面</h3><hr>
<h4 id="1-1-训练-验证-测试集"><a href="#1-1-训练-验证-测试集" class="headerlink" title="1.1 训练 / 验证 / 测试集"></a>1.1 训练 / 验证 / 测试集</h4><p>层数、隐藏单元、学习率、激活函数</p>
<p>这些参数都是需要大迭代更新学习的。</p>
<p>一组数据 = 训练集+ 验证集\交叉验证集 + 测试集    普通量级 70 + 0 + 30     60 + 20 +20  数据量级越大 后两者占比越小 百万级别 98 + 1 + 1</p>
<p><img src="assets/7be8de26db579238f7e72a5a45087595.png" alt="img"></p>
<p><strong>训练集</strong> 加入训练</p>
<p>作用：估计模型，用于训练模型以及确定模型权重</p>
<p>学习样本数据集，通过匹配一些参数来建立一个分类器。建立一种分类的方式，主要是用来训练模型的。</p>
<p><strong>验证集</strong> 加入训练</p>
<p>作用：确定网络结构或者控制模型复杂程度的参数，</p>
<p>对学习出来的模型，调整分类器的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。</p>
<p><strong>测试集 </strong> 不加入训练</p>
<p>作用：检验最终选择最优的模型的性能如何，泛化能力</p>
<p>主要是测试训练好的模型的分辨能力（识别率等）</p>
<p><strong>不要</strong>在训练集和测试集<strong>分布不同</strong>的情况下训练网络，要确保训练集和测试集使用<strong>同一分布</strong></p>
<p><strong>测试集</strong>的目的是让最终所选定的神经网络系统做出<strong>无偏估计</strong>。如果不需要做无偏估计，就不需要测试集。（但比较bug的一点是，这时的验证集又可以被看做是测试集）</p>
<hr>
<h4 id="1-2-偏差-方差"><a href="#1-2-偏差-方差" class="headerlink" title="1.2 偏差 / 方差"></a>1.2 偏差 / 方差</h4><p><img src="assets/image-20210330210829773.png" alt="image-20210330210829773"></p>
<p>高偏差 欠拟合</p>
<p>高方差 过拟合</p>
<p>如果人的误差为 0%</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">训练集误差</th>
<th style="text-align:center">1%</th>
<th style="text-align:center">15%</th>
<th style="text-align:center">15%</th>
<th style="text-align:center">0.5%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">验证集误差</td>
<td style="text-align:center">11%</td>
<td style="text-align:center">16%</td>
<td style="text-align:center">30%</td>
<td style="text-align:center">1%</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">高方差</td>
<td style="text-align:center">高偏差</td>
<td style="text-align:center">高偏差+高方差</td>
<td style="text-align:center">低偏差+低方差</td>
</tr>
</tbody>
</table>
</div>
<p>太线性 高偏差</p>
<p>太非线性 高方差</p>
<p>方差度量随机变量和其数学期望（即均值）之间的偏离程度</p>
<p>方差越大，数据的波动越大，数据分布比较分散</p>
<p>方差越小，数据的波动越小，数据分布比较集中</p>
<hr>
<h4 id="1-3-机器学习基础"><a href="#1-3-机器学习基础" class="headerlink" title="1.3 机器学习基础"></a>1.3 机器学习基础</h4><p>检查偏差，评估训练集性能，如果偏差高（训练集误差大），甚至无法拟合训练集，需要更换一个新的网络（更多层，更多隐藏单元，加大训练时间）</p>
<p> 一旦偏差降到可接受范围，就需要检查方差，评估验证集性能，如果方差高（验证集误差大），正则化/更换网络架构/需要采用更多的数据来训练。</p>
<p>直到找到一个低偏差、低方差的网络。</p>
<p>bias variance trade off 权衡</p>
<hr>
<h4 id="1-4-正则化"><a href="#1-4-正则化" class="headerlink" title="1.4 正则化"></a>1.4 正则化</h4><p><strong>正则化</strong>：多任务学习、增加噪声，集成学习，早停止，稀疏表示，dropout，正切传播，参数绑定，权值共享，范数惩罚，数据增强</p>
<p>如果网络过拟合，即存在高方差的问题。<strong>正则化</strong>可以处理！ </p>
<p><strong>L1范数</strong></p>
<p>向量参数 w 的 L1 范数如下: </p>
<script type="math/tex; mode=display">
\begin{equation}
\|w\|_{1} =\sum_{i=1}^{n_{y}}|w|
\end{equation}</script><p>如果用L1正则化，w最终会是稀疏的。</p>
<p><strong>L2范数</strong></p>
<p>向量参数 w 的 L2 范数，用到了欧几里得法线</p>
<script type="math/tex; mode=display">
\begin{equation}\|\omega\|_{2}^{2}=\sum_{j=1}^{n_{x}} \omega_{j}^{2}=\omega^{\top} \omega\end{equation}</script><p>如果用L2正则化，w的值会比较小，避免过拟合</p>
<p><strong>实例</strong></p>
<p>普通的 logistics 回归任务 $\min _{w, b} J(w, b)$</p>
<script type="math/tex; mode=display">
\begin{equation}J(\omega, b)=\frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|\omega\|_{2}^{2}\end{equation}</script><p>其中 w 和 b 是logistics 的两个参数，$w\in \mathbb{R^{nx}}$， $b\in\mathbb{R}$ </p>
<p>不加 b 的L2范数是因为，w 通常为高维，可以独立表达高偏差问题，b 对模型影响不大，加上也可以。其中 $\lambda$ 是一个需要调整的超参数</p>
<script type="math/tex; mode=display">
J\left(\omega^{[1)}, b^{[1]}, \ldots, \omega^{[L]}, b^{[L]}\right)=\sum_{i=1}^{m} L\left(\hat{y}^{(i)} ,y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\left\|\omega^{[l] }\right\|^{2}_{F}</script><p>其中</p>
<script type="math/tex; mode=display">
\| \omega^{[l]}||^{2}=\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}}\left(\omega_{i j}^{[l]}\right)^{2}_{F}</script><p>矩阵范式，被定义为矩阵中所有元素的平方求和，其中 w 的 size 为 $(n^{[l]},n^{[l-1]})$ </p>
<p>被称为 frobenius 范数/F范数    弗罗贝尼乌斯</p>
<p>参数更新公式为</p>
<script type="math/tex; mode=display">
\begin{split}
w^{[l]}&=w^{[l]}-\alpha(backward)
\end{split}</script><p>如果加上这个正则项，就相当于对梯度 $dW^{[l]}$ 加上了 $\frac{\lambda}{m}w^{[l]}$</p>
<script type="math/tex; mode=display">
\begin{split}
w^{[l]}&=w^{[l]}-\alpha\left[backward+\frac{\lambda}{m} w^{[l]}\right]\\
&=w^{[l]}-\frac{\alpha\lambda}{m}w^{[l]}-\alpha(backward)
\end{split}</script><p>w 的系数是 $(1-\frac{\alpha\lambda}{m})$   <strong>权重衰减</strong>，不论w是什么值，都打算让他变得更小</p>
<hr>
<h4 id="1-5-为什么正则化可以减少过拟合？"><a href="#1-5-为什么正则化可以减少过拟合？" class="headerlink" title="1.5 为什么正则化可以减少过拟合？"></a>1.5 为什么正则化可以减少过拟合？</h4><script type="math/tex; mode=display">
J\left(\omega^{[1)}, b^{[1]}, \ldots, \omega^{[L]}, b^{[L]}\right)=\sum_{i=1}^{m} L\left(\hat{y}^{(i)} ,y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\left\|\omega^{[l] }\right\|^{2}_{F}</script><p>如果将 $\lambda$ 设置足够大，则 w 足够小，许多隐藏单元减少产生的影响，即减少非线性。（考虑极端，如果其他隐藏单元都不产生影响，只有一个有影响，那就是变成线性回归了）</p>
<p><img src="assets/image-20210330221922620.png" alt="image-20210330221922620"></p>
<hr>
<h4 id="1-6-Dropout-正则化"><a href="#1-6-Dropout-正则化" class="headerlink" title="1.6 Dropout 正则化"></a>1.6 Dropout 正则化</h4><p>随机失活，设定节点保留和消除的概率</p>
<p><img src="assets/1617160374854.png" alt="1617160374854"></p>
<p><strong>inverted dropout 反向随机失活</strong></p>
<p>根据阈值，生成一个 bool 类型矩阵，与参数 w 矩阵相乘，得到随机失活后的参数矩阵 w。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = np.random.rand(a.shape[<span class="number">0</span>], a.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a = np.multiply(a, d)</span><br><span class="line">a = a / keep_prob</span><br></pre></td></tr></table></figure>
<p>下一层的计算如下</p>
<script type="math/tex; mode=display">
z = w * a + b</script><p>假设<code>keep_prob = 0.8</code> ，那么有 20% 的数变成了 0，<code>a</code> 的期望变成了原来的 80%， <code>z</code> 的期望也变成了原来的80%。</p>
<p>为了不影响 <code>z</code> 的期望值，需要给 <code>a</code> 除以 0.8 ，修正弥补。</p>
<p><strong>训练测试</strong></p>
<p>不同的训练样本，失活的隐藏单元也不同。每次训练数据的梯度不同，则随机对不同隐藏单元归零。每个 epoch 将不同的隐藏单元归零。  </p>
<p><strong>测试阶段不使用 dropout 函数。也不用除以 <code>keep_prob</code></strong></p>
<hr>
<h4 id="1-7-理解-Dropout"><a href="#1-7-理解-Dropout" class="headerlink" title="1.7 理解 Dropout"></a>1.7 理解 Dropout</h4><p>使用正则化就像是采用一个较小的神经网络。</p>
<p>不愿意吧权重全部放在其中一个特征（输入），因为有可能失活被删除。</p>
<p>实施dropout会减小权重，类似L2正则，预防过拟合。但L2对不同参数的衰减程度不同，</p>
<p>可以针对不同层设置不同的 <code>keep_prob</code></p>
<p>除非算法过拟合（数据不够），否则一般不用dropout</p>
<p>dropout 缺点是损失函数不确定了 。</p>
<p>先关闭 drop 确定损失函数单调递减之后再打开 dropout</p>
<hr>
<h4 id="1-8-其他正则化方法"><a href="#1-8-其他正则化方法" class="headerlink" title="1.8 其他正则化方法"></a>1.8 其他正则化方法</h4><p><strong>数据增强</strong></p>
<p>扩增数据，即加大数据量</p>
<p><img src="assets/1617162083581.png" alt="1617162083581"></p>
<p>水平<strong>翻转</strong>图片，训练集可以大一倍。还处于同一分布。</p>
<p>或<strong>裁剪</strong>图片，但要确保猫还在图片中。</p>
<p><img src="assets/1617162158684.png" alt="1617162158684"></p>
<p>特殊的数据还可以随意轻微的<strong>旋转</strong>或<strong>扭曲</strong>。</p>
<p><strong>early stopping</strong></p>
<p>绘制训练误差和验证误差。验证集误差通常是先变小后变大。在最小值就可以提前停止训练了。</p>
<p>训练神经网络之前，w 很小，在迭代过程中 w 会越来越大。</p>
<p>但是提前停止训练，不能独立的处理代价函数 J 和验证集误差。了解即可，不建议用。</p>
<p><strong>解决办法就是给J加上正则化项，并不使用early stopping</strong>，这会使超参数搜索空间更容易分解，更容易搜索。缺点就是需要调整正则化参数 $\lambda$</p>
<hr>
<h4 id="1-9-归一化输入"><a href="#1-9-归一化输入" class="headerlink" title="1.9 归一化输入"></a>1.9 归一化输入</h4><p>normalizing </p>
<p><img src="assets/1617165180456.png" alt="1617165180456"></p>
<p><strong>第一步 零均值化</strong></p>
<p><img src="assets/1617165195446.png" alt="1617165195446"></p>
<script type="math/tex; mode=display">
\begin{equation}
  \mu=\frac{1}{m} \sum_{i=1}^{m} x^{(i)} \\
 x = x -\mu
\end{equation}</script><p>相当于移动数据集。</p>
<p><strong>第二步 归一化方差</strong></p>
<p><img src="assets/1617167345080.png" alt="1617167345080"></p>
<script type="math/tex; mode=display">
\begin{equation}
 \sigma^{2}=\frac{1}{m} \sum_{i=1}^{M} x^{(i)} * * 2 \\
 x = x / \sigma
\end{equation}</script><p><code>**2</code> 表示每个元素都平方 element-wise</p>
<p>$\sigma^2$ 是一个向量，它的每个特征都有方差，因为均值已经为 0，所以 x 的平方直接就是方差。</p>
<p><strong>注意</strong></p>
<p>也应该用<strong>从训练集计算得到的参数</strong>处理<strong>测试集</strong>。</p>
<p><img src="assets/1617167554679.png" alt="1617167554679"></p>
<p><img src="assets/1617167574641.png" alt="1617167574641"></p>
<p>比较狭长。梯度下降可能拐来拐去，才能到最优值。</p>
<p><img src="assets/1617167588247.png" alt="1617167588247"></p>
<p><img src="assets/1617167613864.png" alt="1617167613864"></p>
<p>比较均匀。梯度下降法直接指向最小值，能用较大步长。</p>
<hr>
<h4 id="1-10-梯度消失与梯度爆炸"><a href="#1-10-梯度消失与梯度爆炸" class="headerlink" title="1.10 梯度消失与梯度爆炸"></a>1.10 梯度消失与梯度爆炸</h4><p><img src="assets/1617167911663.png" alt="1617167911663"></p>
<p>参数比1大，随层数 L 指数增长。参数比1小，随层数 L 指数递减</p>
<hr>
<h4 id="1-11-神经网络的权重初始化"><a href="#1-11-神经网络的权重初始化" class="headerlink" title="1.11 神经网络的权重初始化"></a>1.11 神经网络的权重初始化<span id="winit"></span></h4><p><img src="assets/1617245938449.png" alt="1617245938449"></p>
<p>其中有n个特征</p>
<script type="math/tex; mode=display">
\begin{equation}
 z=\omega_{1} x_{1}+\omega_{2} x_{2}+\cdots \cdot + \omega_{n} x_{n} 
\end{equation}</script><p>因为 z 是求和，所以 n 越大，z 越大</p>
<p>合理的方法是设置</p>
<script type="math/tex; mode=display">
W=\frac{1}{n}W</script><p>即令其<strong>方差</strong>为 $\frac{1}{n}$，即上一层的特征数量的平方根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>]) * np.sqrt(<span class="number">1</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>
<p>如果是 <code>ReLu</code> 函数，令其<strong>方差</strong>为 $\frac{2}{n}$，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])  * np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>
<p>如果是 <code>Tanh</code> 函数，令其<strong>方差</strong>为 $\frac{1}{n}$。Xavier 初始化</p>
<blockquote>
<p>x ~ N(μ, σ²)                  kx ~ N(kμ, k²σ²)</p>
<p>整个大型前馈神经网络无非就是一个超级大映射，将原始样本<strong>稳定的</strong>映射成它的类别。也就是将样本空间映射到类别空间。</p>
<p>如果样本空间与类别空间的<strong>分布差异</strong>很<strong>大</strong>，比如说<strong>类别空间特别稠密</strong>，<strong>样本空间特别稀疏辽阔</strong>，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。同样，如果<strong>类别空间特别稀疏</strong>，<strong>样本空间特别稠密</strong>，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。</p>
<p>因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，<strong>也就是要让它们的方差尽可能相等</strong>。</p>
</blockquote>
<hr>
<h4 id="1-12-梯度的数值逼近"><a href="#1-12-梯度的数值逼近" class="headerlink" title="1.12 梯度的数值逼近"></a>1.12 梯度的数值逼近</h4><p>在反向传播时，有一步梯度检验。对计算数值做逼近</p>
<p>双边误差，更逼近导数，误差量级更小，结果更准确。</p>
<script type="math/tex; mode=display">
\begin{equation}
 f^{\prime}(\theta)=\lim _{\varepsilon \rightarrow 0} \frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2 \varepsilon} 
\end{equation}</script><p>逼近误差为 $O(\epsilon^2)$</p>
<hr>
<h4 id="1-13-梯度检验"><a href="#1-13-梯度检验" class="headerlink" title="1.13 梯度检验"></a>1.13 梯度检验</h4><p>计算梯度的数值逼近 $d\theta_{approx}$ 和数值解 $d\theta$，比较二者差距。使用欧几里得范数</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{\left\|d\theta_{approx}-d \theta\right\|_{2}}{\left\|d\theta_{approx}\right\|_{2} +
\left\|d \theta\right\|_{2} }
\end{equation}</script><p>误差平方和然后在求平方根，得到欧氏距离。分母预方向量太大或太小。分母为两个参数向量的模之和。</p>
<p>三角形两边之和大于第三边，确保上式落在 $[0, 1]$ 之间。</p>
<p>如果值小于 $1e-7$ 或更小即通过验证。</p>
<p>如果大于 $1e-5$ ，就需要检查有没有其中一项的误差，即两者差值特别大。</p>
<hr>
<h4 id="1-14-关于梯度检验实现的注记"><a href="#1-14-关于梯度检验实现的注记" class="headerlink" title="1.14 关于梯度检验实现的注记"></a>1.14 关于梯度检验实现的注记</h4><ol>
<li>不要在训练中使用梯度检验，只用于debug</li>
<li>如果梯度检验失败。检查每一项逼近解 $d\theta_{approx}$ 和数值解 $d\theta$ 的插值，寻找哪一项误差最大。如果发现某层的 $d\theta^{[l]}$ 特别大，但是 $dw^{[l]}$ 的各项非常接近，那么一定是在计算参数 $b$ 的导数 $db^{[l]}$ 存在bug。同理，如果发现某层的 $d\theta^{[l]}$ 特别大，但是 $db^{[l]}$ 的各项非常接近，那么一定是在计算参数 $w$ 的导数 $dw^{[l]}$ 存在bug。</li>
<li>注意正则化项，要包含进去</li>
<li>不能与dropout共同使用</li>
<li>如果碰到当 $w$ 和 $b$ 接近0时，检验正确；但是训练过程， $w$ 和 $b$ 逐渐变大，检验不通过。可以在一开始做一下梯度检验，训练一段时间后再进行一次梯度检验确保正确。</li>
</ol>
<hr>
<h3 id="第二周-优化算法"><a href="#第二周-优化算法" class="headerlink" title="第二周 优化算法"></a>第二周 优化算法</h3><h4 id="2-1-Mini-batch-梯度下降法"><a href="#2-1-Mini-batch-梯度下降法" class="headerlink" title="2.1 Mini-batch 梯度下降法"></a>2.1 Mini-batch 梯度下降法</h4><p>特征 X 的 size 为 $[x_n, m]$，标签 Y 的 size 为 $[1,m]$.</p>
<p>如果样本数 m 巨大如 5000000个，训练速度很慢。因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。如果先让梯度下降法处理一部分，算法速度会很快。</p>
<p>可以把训练集<strong>分割</strong>为小一点的子训练集 如1000个一组。子集被称为 <strong>mini-batch</strong>。如下共有 5000 个 mini-batch。</p>
<p><img src="assets/equation.svg" alt="[公式]"></p>
<p><img src="assets/equation-1617340169208.svg" alt="[公式]"></p>
<p>一个 mini-batch 的特征 X 的 size 为 $[x_n, 1000]$，标签 Y 的 size 为 $[1, 1000]$.</p>
<p><img src="assets/1617370541220.png" alt="1617370541220"></p>
<p>把所有训练集完整的遍历完为 1 个 epoch。采用 mini-batch 方法 1 个 epoch 梯度下降 5000 次，否则 1 个 epoch 只下降一次。</p>
<hr>
<h4 id="2-2-理解-mini-batch-梯度下降法"><a href="#2-2-理解-mini-batch-梯度下降法" class="headerlink" title="2.2 理解 mini-batch 梯度下降法"></a>2.2 理解 mini-batch 梯度下降法</h4><p><img src="assets/20171026113219156" alt="这里写图片描述"></p>
<p>mini-batch 梯度不是每次都在下降的。但如果数据足够均匀，右图噪声就越小。</p>
<p>如果mini-batch size =  m （样本数 ），称为batch 梯度下降，只有一个子集就是其本身</p>
<p>如果mini-batch size =  1  ，称为随机梯度下降，每个样本都是一个 mini-batch</p>
<p>随机梯度永远不会收敛，在最小值附近波动</p>
<p><img src="assets/20171026135131370" alt="这里写图片描述"></p>
<p>蓝色的线代表 Batch gradient descent，紫色的线代表 Stachastic gradient descent。Batch gradient descent会比较<strong>平稳</strong>地接近全局最小值，但是因为使用了所有m个样本，每次前进的<strong>速度</strong>有些慢。Stachastic gradient descent每次前进<strong>速度</strong>很快，但是路线曲折，有较大的<strong>振荡</strong>，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就<strong>不能</strong>使用<strong>向量化</strong>的方法来提高运算速度。</p>
<p>因此要设置一个合适的 mini-batch size</p>
<p>一般来说，如果总体样本数量 $m$ 不太大时，例如 $m≤2000$，建议直接使用Batch gradient descent。</p>
<p>如果总体样本数量 $m$ 很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为 64,128,256,512。这些都是2的幂。之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。</p>
<hr>
<h4 id="2-3-指数加权平均"><a href="#2-3-指数加权平均" class="headerlink" title="2.3 指数加权平均"></a>2.3 指数加权平均</h4><script type="math/tex; mode=display">
\begin{equation}
 V_{t}=\beta V_{t-1}+(1-\beta) \theta_{t} 
\end{equation}</script><ul>
<li><strong>large β:</strong> 适应的慢，相对平滑</li>
<li><strong>small β:</strong> 噪声更大，不平滑</li>
</ul>
<p>如果参数设置为 0.9，即第 $t$ 天与第 $t-1$ 天的气温迭代关系为：</p>
<script type="math/tex; mode=display">
\begin{equation}
 \begin{aligned} V_{t} &=0.9 V_{t-1}+0.1 \theta_{t} \\ &=0.9^{t} V_{0}+0.9^{t-1} \cdot 0.1 \theta_{1}+0.9^{t-2} \cdot 0.1 \theta_{2}+\cdots+0.9 \cdot 0.1 \theta_{t-1}+0.1 \theta_{t} \end{aligned} 
\end{equation}</script><p>$β$ 值决定了指数加权平均的天数，近似表示为：</p>
<script type="math/tex; mode=display">
\frac{1}{1-\beta}</script><p>指数加权移动平均值</p>
<p>指数加权平均数 </p>
<hr>
<h4 id="2-4-理解指数加权平均"><a href="#2-4-理解指数加权平均" class="headerlink" title="2.4 理解指数加权平均"></a>2.4 理解指数加权平均</h4><p><img src="assets/1617372894803.png" alt="1617372894803"></p>
<script type="math/tex; mode=display">
\begin{equation}
 \begin{aligned} V_{t}=& \beta V_{t-1}+(1-\beta) \theta_{t} \\=&(1-\beta) \theta_{t}+(1-\beta) \cdot \beta \cdot \theta_{t-1}+(1-\beta) \cdot \beta^{2} \cdot \theta_{t-2}+\cdots \\ &+(1-\beta) \cdot \beta^{t-1} \cdot \theta_{1}+\beta^{t} \cdot V_{0} \end{aligned} 
\end{equation}</script><p>观察上面这个式子，$ \theta_{t}, \theta_{t-1}, \theta_{t-2}, \cdots, \theta_{1} $ 是原始数据值，$ (1-\beta),(1-\beta) \beta,(1-\beta) \beta^{2}, \cdots,(1-\beta) \beta^{t-1} $ 是类似指数曲线，从右向左，呈指数下降的。$V_t$ 的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。</p>
<p><img src="assets/20171027155944527" alt="这里写图片描述"></p>
<p>实践中，经常vθ初始化为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vθ = <span class="number">0</span></span><br><span class="line">repeat:</span><br><span class="line">    get <span class="built_in">next</span> θt</span><br><span class="line">    vθ = β * vθ + (<span class="number">1</span>-β) * θt</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="2-5-指数加权平均的偏差修正"><a href="#2-5-指数加权平均的偏差修正" class="headerlink" title="2.5 指数加权平均的偏差修正"></a>2.5 指数加权平均的偏差修正</h4><p>上文中提到当 $β=0.98$ 时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。</p>
<p><img src="assets/20171028095447301" alt="这里写图片描述"></p>
<p>我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置 $V_0=0$，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。</p>
<p>修正这种问题的方法是进行偏移校正（bias correction），即在每次计算完 $V_t$ 后，对 $V_t$ 进行下式处理：</p>
<script type="math/tex; mode=display">
\begin{equation}
 \frac{V_{t}}{1-\beta^{t}} 
\end{equation}</script><p>在刚开始的时候，$t$ 比较小，$(1−β^t)&lt;1$，这样就将 $V_t$ 修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着 $t$ 增大，$(1−β^t)≈1$ ，$V_t$ 基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。</p>
<p>机器学习中，偏移校正并不是必须的。因为，在迭代一次次数后（$t$ 较大），$V_t$ 受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以<strong>忽略初始迭代过程</strong>，等到一定迭代之后再取值，这样就不需要进行偏移校正了。</p>
<hr>
<h4 id="2-6-动量梯度下降法"><a href="#2-6-动量梯度下降法" class="headerlink" title="2.6 动量梯度下降法"></a>2.6 动量梯度下降法</h4><blockquote>
<p>momentum：基本的想法是计算梯度的指数加权平均数，并利用该梯度更新你的权重</p>
</blockquote>
<p>梯度下降法，要很多计算步骤，慢慢摆动到最小值。摆动减慢了梯度下降的速度，无法使用较大的学习率。</p>
<p><img src="assets/20171028142838569" alt="这里写图片描述"></p>
<p>希望纵向学习率低一点，减少摆动。横向学习率高一点，快速靠近最小值。采用</p>
<script type="math/tex; mode=display">
\begin{equation}
 v_{d W}=\beta v_{d W} +(1-\beta) d W =\beta v_{d W} +W
\end{equation}</script><p>即旧的惯性加新的方向。指数加权平均，减少纵轴波动，相当于取平均且平均为 0，正负数相互抵消。而所有的微分都指向横轴方向。momentum项 $v_{dW}$ 提供速度，微分项 $dW$ 提供加速度。$\beta$ 相当于提供摩擦力，不让无限加速下去。</p>
<p><img src="assets/1617433470211.png" alt="1617433470211"></p>
<p>有两个超参数 $\alpha,\beta$，$\beta$ 常用 0.9 是很棒的鲁棒数。一般不适用偏差干扰项。且有时 $1-\beta$ 项会去掉，一般不去比较好，因为会影响 $\alpha$ 的值。</p>
<hr>
<h4 id="2-7-RMSprop"><a href="#2-7-RMSprop" class="headerlink" title="2.7 RMSprop"></a>2.7 RMSprop</h4><p> root mean square prop 均方根，因为你将微分进行平方，然后最后使用平方根。</p>
<script type="math/tex; mode=display">
 S_{W}=\beta S_{d W}+(1-\beta) d W^{2} \\
 S_{b}=\beta S_{d b}+(1-\beta) d b^{2} \\
 W:=W-\alpha \frac{d W}{\sqrt{S_{W}}}\\
 b:=b-\alpha \frac{d b}{\sqrt{S_{b}}}</script><p>从下图中可以看出，梯度下降（蓝色折线）在垂直方向（b）上<strong>振荡较大</strong>，在水平方向（W）上振荡较小，表示在b方向上<strong>梯度较大</strong>，即 $db$ 较大，而在 W 方向上梯度较小，即 $dW$ 较小。因此，上述表达式中 $S_b$ 较<strong>大</strong>，而 $S_W$ 较小。在更新 W 和 b 的表达式中，变化值 $ \frac{d W}{\sqrt{S W}} $ 较大，而 $ \frac{d b}{\sqrt{S_{b}}} $ 较<strong>小</strong>。也就使得 W 变化得多一些，b 变化得<strong>少</strong>一些。</p>
<p><img src="assets/20171028163526337" alt="这里写图片描述"></p>
<p>即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。</p>
<p>避免 RMSprop 算法中分母为零，通常可以在分母增加一个极小的常数 $ε$ ：</p>
<script type="math/tex; mode=display">
\begin{equation}
 W:=W-\alpha \frac{d W}{\sqrt{S_{W}}+\varepsilon}, b:=b-\alpha \frac{d b}{\sqrt{S_{b}}+\varepsilon} 
\end{equation}</script><p>其中，$ε=10^{−8}$，或者其它较小值。</p>
<hr>
<h4 id="2-8-Adam-优化算法"><a href="#2-8-Adam-优化算法" class="headerlink" title="2.8 Adam 优化算法"></a>2.8 Adam 优化算法</h4><p>2015年 ICLR 提出的 A method for Stochastic Optimization, that the name is derived from adaptive moment estimation</p>
<p>Stochastic Optimization 随机优化</p>
<p>derived from 来源于</p>
<p>adaptive moment estimation 自适应矩估计</p>
<p>Init:  $V_{dW}=0, S_{dW},\space\space V_{db}=0, S_{db}=0$<br>On iteration t:<br>    Compute $dW,\space\space db$<br>    $ V_{d W}=\beta_{1} V_{d W}+\left(1-\beta_{1}\right) d W,\space\space V_{d b}=\beta_{1} V_{d b}+\left(1-\beta_{1}\right) d b $<br>    $ S_{d W}=\beta_{2} S_{d W}+\left(1-\beta_{2}\right) d W^{2},\space\space S_{d b}=\beta_{2} S_{d b}+\left(1-\beta_{2}\right) d b^{2} $<br>    Compute bias corrected 偏差修正<br>    $ V_{d W}^{\text {corrected }}=\frac{V_{d W}}{1-\beta_{1}^{t}},\space\space V_{d b}^{\text {corrected }}=\frac{V_{d b}}{1-\beta_{1}^{t}} $<br>    $ S_{d W}^{\text {corrected }}=\frac{S_{d W}}{1-\beta_{2}^{t}},\space\space S_{d b}^{\text {corrected }}=\frac{S_{d b}}{1-\beta_{2}^{t}} $<br>    $ W:=W-\alpha \frac{V_{d W}^{\text {corrected }}}{\sqrt{S_{d W}^{\text {Corrected }}}},\space\space b:=b-\alpha \frac{V_{d b}^{\text {corrected }}}{\sqrt{S_{d b}^{\text {corrected }}}} $</p>
<p>计算<strong>Momentum</strong>指数加权平均数，用<strong>RMSprop</strong>进行更新。其中 $dW^2$ $db^2$ 是对整个积分进行平方（element-wise）。偏差修正时的 t 表示迭代次数。</p>
<p>Adam算法包含了几个超参数，分别是：$α,β_1,β_2,ε$。其中，$β_1$ 通常设置为0.9，$β_2$ 通常设置为0.999，$ε$ 通常设置为 $10^{−8}$。一般只需要对 $β_1$ 和 $β_2$ 进行调试。</p>
<p>实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。</p>
<p>adaptive moment estimation 自适应矩估计</p>
<p>$β_1$ 用来计算微分 $dW$，叫做第一矩</p>
<p>$β_2$ 用来计算平方数的指数加权平均数 $dW^2$ ，叫做第二矩。</p>
<p>论文中的算法</p>
<p><img src="assets/v2-5db14a3057bc9c9c407ede98f14eb6f6_720w.jpg" alt="img"></p>
<hr>
<h4 id="2-9-学习率衰减"><a href="#2-9-学习率衰减" class="headerlink" title="2.9 学习率衰减"></a>2.9 学习率衰减</h4><p>learning rate decay</p>
<p>加快深度学习训练速度的一个办法，随时间慢慢减小学习率。</p>
<p>如果学习率 $\alpha$ 是固定的值，且batch较小，算法不会收敛，只会在最优解附近不断徘徊。 </p>
<p>下图中，蓝色折线表示使用恒定的学习因子 $α$，由于每次训练 $α$ 相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的 $α$，随着训练次数增加，$α$ 逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的 $α$ 来说，learning rate decay 更接近最优值。</p>
<p>1 epoch = 1 pass through datasets</p>
<p>遍历一次数据集</p>
<script type="math/tex; mode=display">
\begin{equation}
 \alpha=\frac{1}{1+\underbrace{\text { decay-rate }}_{\text {hyperparameter }} \times \text { epoch }} \cdot \alpha_{0} 
\end{equation}</script><p>指数下降</p>
<script type="math/tex; mode=display">
\begin{equation}
 \alpha=\lambda^{\text {epoch-number }} \cdot \alpha_{0}, \quad \lambda<1 \sim 0.95 
\end{equation}</script><p>对数下降</p>
<script type="math/tex; mode=display">
\begin{equation}
 \alpha=\frac{\overbrace{\gamma_{c o n s t}}^{\text {hyperparameter }}}{\sqrt{\text { epoch-number }}} \cdot \alpha_{0} \quad  or  \quad=\frac{\gamma_{\text {const }}}{\sqrt{t}} \cdot \alpha_{0} 
\end{equation}</script><p>离散阶梯</p>
<script type="math/tex; mode=display">
\begin{equation}
 \alpha=f_{\text {discrete staircase }} 
\end{equation}</script><hr>
<h4 id="2-10-局部最优的问题"><a href="#2-10-局部最优的问题" class="headerlink" title="2.10 局部最优的问题"></a>2.10 局部最优的问题</h4><p><strong>只要选择合理的强大的神经网络，一般不太可能陷入局部最优</strong></p>
<p>平稳段是一块区域导数长期为0，会降低学习速度。</p>
<p>如果都是凸函数，就很好求解。</p>
<p>马鞍面，一凸一凹组成的，交点为鞍点。</p>
<hr>
<h3 id="第三周-超参数调试、Batch-正则化和程序框架"><a href="#第三周-超参数调试、Batch-正则化和程序框架" class="headerlink" title="第三周 超参数调试、Batch 正则化和程序框架"></a>第三周 超参数调试、Batch 正则化和程序框架</h3><h4 id="3-1-调试处理"><a href="#3-1-调试处理" class="headerlink" title="3.1 调试处理"></a>3.1 调试处理</h4><p>T3 学习率 最重要</p>
<p>T2 动量梯度下降因子 隐藏单元 mini-batch size </p>
<p>T1 网络层数，学习率衰减因子 </p>
<p>T0 Adam算法参数</p>
<p>参数较少时，可以全部试一遍</p>
<p>参数较多时，随机采样。<strong>coarse to fine</strong> 由粗糙到精细的搜索。即采样后，也许你会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域，然后在其中更密集得取值或随机取值，聚集更多的资源。</p>
<hr>
<h4 id="3-2-为超参数选择合适的范围"><a href="#3-2-为超参数选择合适的范围" class="headerlink" title="3.2 为超参数选择合适的范围"></a>3.2 为超参数选择合适的范围</h4><p>参数在不同的数量级，对变化的敏感程度不一样，取<strong>对数</strong> log10，就是在数量级上均匀取值（分布），能快速确定数量级大小</p>
<p>$α \in[ 10^a ~ 10^b]$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = np.random.uniform(a, b)</span><br><span class="line">alpha = <span class="number">10</span> ** r</span><br></pre></td></tr></table></figure>
<p><img src="assets/20171101094754376" alt="这里写图片描述"></p>
<p>$β \in[0.9 ~ 0.999] → [1-10^{b} ~ 1-10^{a}]$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r = np.random.uniform(a, b)</span><br><span class="line">beta = <span class="number">1</span> - <span class="number">10</span> ** r</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="3-3-超参数训练的实践：Pandas-VS-Caviar"><a href="#3-3-超参数训练的实践：Pandas-VS-Caviar" class="headerlink" title="3.3 超参数训练的实践：Pandas VS Caviar"></a>3.3 超参数训练的实践：Pandas VS Caviar</h4><p><strong>pandas</strong></p>
<p>照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的CPU和GPU的前提下，基本而言，你只可以一次负担起试验一个模型或一小批模型。</p>
<p>比如，第0天，你将随机参数初始化，然后开始试验，然后你逐渐观察自己的模型评价曲线，在第1天内逐渐减少，那这一天末的时候，试着增加一点学习速率，看看它会怎样，也许结果证明它做得更好。两天后，它依旧做得不错，也许我现在可以填充下Momentum或减少变量。第三天，发现你的学习率太大了，所以你可能又回归之前的模型。</p>
<p>每天花时间照看此模型，即使是它在许多天或许多星期的试验过程中。所以这是一个人们照料一个模型的方法，观察它的表现，耐心地调试学习率。</p>
<p><strong>caviar</strong></p>
<p>同时试验多种模型，你设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后会获得多条模型评价曲线。最后快速选择工作效果最好的那个。</p>
<hr>
<h4 id="3-4-归一化网络的激活函数"><a href="#3-4-归一化网络的激活函数" class="headerlink" title="3.4 归一化网络的激活函数"></a>3.4 归一化网络的激活函数</h4><p>对当前层的每个样本计算出的隐藏单元值进行归一化，共有m个样本(= mini batch_size)</p>
<p>已知第 l 层的隐藏单元值为：$ z^{<a href="i">l</a>}=z^{(1)}, z^{(2)}, \ldots, z^{(m)} $</p>
<p>归一化：</p>
<script type="math/tex; mode=display">
\begin{equation}
 \begin{aligned} 
 \mu &=\frac{1}{m} \sum_{i=1}^{m} z^{(i)} \\ 
 \sigma^{2} &=\frac{1}{m} \sum_{i=1}^{m}\left(z^{(i)}-\mu\right)^{2} \\ 
 z_{\text {norm }}^{(i)} &=\frac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}
 \end{aligned} 
\end{equation}</script><p>其中 $\varepsilon$ 防止分母为0，取值 $10^{-8}$。这样该隐藏层的所有输入 $z^{(i)}$ 均值为0，方差为1。</p>
<p>但是，大部分情况下并不希望所有的  $z^{(i)}$ 均值都为 0，方差都为 1，也不太合理。通常需要对  $z^{(i)}$  进行进一步处理：</p>
<script type="math/tex; mode=display">
\tilde{z}^{(i)} =\gamma z_{\text {norm }}^{(i)}+\beta</script><p>其中 $\gamma,\beta$ 是需要学习的参数，可以通过梯度下降等算法求得。这里， $\gamma,\beta$ 的作用是让 $ \tilde{z}^{(i)} $ 的均值和方差为任意值，只需调整其值就可以了。特别的如果：</p>
<script type="math/tex; mode=display">
\begin{equation}
 \gamma=\sqrt{\sigma^{2}+\varepsilon}, \beta=u 
\end{equation}</script><p>则有 $ \tilde{z}^{(i)} = z^{(i)} $。通过Batch Normalization，对隐藏层的各个$z^{<a href="i">l</a>}$ 进行归一化处理，且下一层的输入为 $ \tilde{z}^{<a href="i">l</a>} $ ，而不是 $ z^{<a href="i">l</a>} $</p>
<p>输入的标准化处理 Normalizing inputs 和隐藏层的标准化处理 Batch Normalization 是<strong>有区别的</strong>。Normalizing inputs 使所有输入的均值为0，方差为1。而 Batch Normalization 可使各隐藏层输入的均值和方差为任意值。</p>
<p><img src="assets/2d314293ae9aaa67285299267857632f.png" alt="img"></p>
<p>实际上，从激活函数的角度来说，如果各隐藏层的输入<strong>均值</strong>在靠近 <strong>0</strong> 的区域即处于激活函数的<strong>线性</strong>区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。这也解释了为什么需要用  $\gamma,\beta$  来对 $z^{<a href="i">l</a>}$ 作进一步处理。</p>
<hr>
<h4 id="3-5-将-Batch-Norm-拟合进神经网络"><a href="#3-5-将-Batch-Norm-拟合进神经网络" class="headerlink" title="3.5 将 Batch Norm 拟合进神经网络"></a>3.5 将 Batch Norm 拟合进神经网络</h4><p>batch norm 在激活函数前进行</p>
<p>全连接网络中共有 N <em> L </em> 2 个 BN 参数，L 表示层数，N 表示一层里隐藏单元数目。</p>
<p><img src="assets/equation-1617519745354.svg" alt="[公式]"></p>
<p>如果使用 BN，那么 bias 即 $b$ 可以去除掉，因为要先将 $z^{[L]}$ 减去均值，而 bias 会被均值减法抵消掉。其实 $\beta$ 就把 $b$ 包括进去了，二者都在调整这一特征的平均的水平</p>
<script type="math/tex; mode=display">
\begin{equation}
 \begin{aligned} 
 z^{[l]}&=W^{[l]} a^{[l-1]} \\
 z_{norm}^{[l]} &= \frac{z^{[l]}-\mu}{\sqrt{\sigma^{2}+\varepsilon}} \\
 \tilde{z}^{[l]}&=\gamma^{[l]} z_{\text {norm }}^{[l]}+\beta^{[l]} 
 \end{aligned}
\end{equation}</script><p>Parameters: $ W^{[1]} \in \mathbb{R}^{n[l] \times n^{[l-1]}}, \quad \gamma^{[l]} \in \mathbb{R}^{n^{[l]} \times 1}, \quad \beta^{[l]} \in \mathbb{R}^{n^{[l]} \times 1} $</p>
<p>for t = 1, 2, …, num_mini_batches<br>    forward prop on $X^{\{t\}}$<br>    in each hidden layer, use BN to replace $z^{[l]}$ with $ \tilde{z}^{[l]}$<br>    back prop to compute $dW^{[l]}, dγ^{[l]}, dβ^{[l]}$<br>    update parameters</p>
<script type="math/tex; mode=display">
\begin{equation}
 W^{[l]}:=W^{[l]}-\alpha d W^{[l]} \\
 \gamma^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]} \\
 \beta^{[l]}:=\beta^{[l]}- \alpha d\beta^{[l]} 
\end{equation}</script><p>works with Momentum / RMSProp / Adam</p>
<hr>
<h4 id="3-6-Batch-Norm-为什么奏效？"><a href="#3-6-Batch-Norm-为什么奏效？" class="headerlink" title="3.6 Batch Norm 为什么奏效？"></a>3.6 Batch Norm 为什么奏效？</h4><blockquote>
<p>covariate shift：两组数据分布不一致，但条件分布一致</p>
</blockquote>
<p>BN 减少了隐藏值分布变化的数量。哪怕前一层参数变换，BN后的均值和方差都一样（分布很相同）。</p>
<p>限制了前层的<strong>参数更新</strong>会<strong>影响</strong>数值<strong>分布</strong>的程度。</p>
<p>Batch Norm 减少了各层 $W^{[l]}$、$B^{[l]}$ 之间的<strong>耦合性</strong>，让各层更加<strong>独立</strong>，实现自我训练学习的效果。</p>
<p>对于特别多层的网络，后面的累积<strong>分布差异</strong>跟原数据分布完全不一样。每换一个batch，分布又可能往另一种方式差异化。BN 解决了这个问题。</p>
<p>简单的说就是让各层之间相对<strong>独立</strong>，不会因为前面层的变动导致后面层巨大变化。</p>
<p>像是把一个工序相互影响很大的工厂变成流水线，当前一层只需要考虑上一层的结果和当前层的处理。上一层的结果（分布）也比较稳定，当前层做起来就比较轻松。</p>
<p>BN 一次只能针对一个 mini-batch，每个mini-batch都有一个均值和方差，而不是用整个数据集计算。因此会产生 noise，迫使后边网络，不过分依赖于任何一个隐藏单元，slight 正则化效果。</p>
<hr>
<h4 id="3-7-测试时的-Batch-Norm"><a href="#3-7-测试时的-Batch-Norm" class="headerlink" title="3.7 测试时的 Batch Norm"></a>3.7 测试时的 Batch Norm<span id ="BN-test"></span></h4><p>BN 将你的数据以mini-batch的形式逐一处理，但在测试时，你可能需要对每个样本逐一处理</p>
<p>为了将你的神经网络运用于测试，就需要单独估算 $\mu$ 和 $\sigma^2$.</p>
<p>可以把所有训练集放入最终的神经网络模型中，然后直接计算每层的参数。</p>
<p>也可以用指数加权平均。</p>
<hr>
<h4 id="3-8-Softmax-回归"><a href="#3-8-Softmax-回归" class="headerlink" title="3.8 Softmax 回归"></a>3.8 Softmax 回归</h4><p>传统logistic回归为 神经网络输出层只有一个神经元，表示预测输出 $\hat{y}$ 是正类的概率$ P(y=1 \mid x)$，$\hat{y}&gt;0.5 $ 则判断为正类，$\hat{y}&lt;0.5 $ 则判断为负类。</p>
<p>Softmax 处理多分类任务。神经网络中输出层就有C个神经元，即 $n^{[L]} = C$。每个神经元的输出依次对应属于该类的概率 $ P(y=C \mid x)$</p>
<p>最后一层输出    </p>
<script type="math/tex; mode=display">
\begin{equation}
 z^{[L]}=W^{[L]} a^{[L-1]}+b^{[L]} 
\end{equation}</script><p>通过Softmax激活函数</p>
<script type="math/tex; mode=display">
 t=e^{z^{[L]}}, \quad t \in \mathbb{R}^{4 \times 1} \\ a^{[L]}=\frac{e^{z^{[L]}}}{\sum_{i=1}^{4} t_{i}}, \quad a_{i}^{[L]}=\frac{t_{i}}{\sum_{i=1}^{4} t_{i}} \\ \operatorname{softmax}: a_{(c, 1)}^{[L]}=g^{[L]}\left(z_{\{c, 1\rangle}^{[L]}\right)</script><p>其中 C 表示分类数量，例子中 C = 4，a 表示对应所属类的概率，维度与 z 相同。且</p>
<script type="math/tex; mode=display">
\begin{equation}
 \sum_{i=1}^{C} a_{i}^{[L]}=1 
\end{equation}</script><p>Softmax 回归是 logistic 回归的一般形式。Softmax回归 = 分C类的广义逻辑回归</p>
<p><img src="assets/v2-11758fbc2fc5bbbc60106926625b3a4f_1440w.jpg" alt="详解softmax函数以及相关求导过程"></p>
<hr>
<h4 id="3-9-训练一个-Softmax-分类器"><a href="#3-9-训练一个-Softmax-分类器" class="headerlink" title="3.9 训练一个 Softmax 分类器"></a>3.9 训练一个 Softmax 分类器<span id ="dadz"></span></h4><p>让 $\hat{y}2$ 尽可能大，除了第二项其余项都为零</p>
<script type="math/tex; mode=display">
\begin{equation}
 y=\left[\begin{array}{l}0 \\ 1 \\ 0 \\ 0\end{array}\right] \quad a^{[L]}=\hat{y}=\left[\begin{array}{c}0.3 \\ 0.2 \\ 0.1 \\ 0.4\end{array}\right] \quad C=4 \\
 L(\hat{y}, y)=-\sum^{4} y_{j} \log \hat{y}_{j}=-\log \hat{y}_{2} \Rightarrow \hat{y}_{2} \uparrow 
\end{equation}</script><p>cost函数为</p>
<script type="math/tex; mode=display">
\begin{equation}
 J\left(W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right) 
\end{equation}</script><p>损失函数为</p>
<script type="math/tex; mode=display">
\begin{equation}
 L(\hat{y}, y)=-\sum_{j=1}^{4} y_{j} \log \hat{y}_{j} 
\end{equation}</script><p>其中</p>
<script type="math/tex; mode=display">
da = \frac{\part L}{\part a}=-\frac{y}{\hat{y}}=-\frac{1}{\hat{y}}</script><p>因为 y 的值只有 0/1，0 项消掉了，只剩 1 项。</p>
<p>激活函数</p>
<script type="math/tex; mode=display">
\begin{equation}
 t=e^{z^{[L]}}, \quad t \in \mathbb{R}^{C \times 1} \\ a^{[L]}=\frac{e^{z^{[L]}}}{\sum_{i=1}^{C} t_{i}}, \quad a_{i}^{[L]}=\frac{t_{i}}{\sum_{i=1}^{C} t_{i}} \\ \operatorname{softmax}: a_{(c, 1)}^{[L]}=g^{[L]}\left(z_{\{c, 1\rangle}^{[L]}\right) 
\end{equation}</script><p>其中</p>
<script type="math/tex; mode=display">
\frac{\part a}{\part z} =  \frac{\partial}{\partial z} \cdot\left(\frac{e^{z_{i}}}{\sum_{i=1}^{C} e^{z_{i}}}\right) \\
= a\cdot(1-a)</script><p>得</p>
<script type="math/tex; mode=display">
dz=\frac{\part L}{\part z} \\
= \frac{\part L}{\part a}\frac{\part a}{\part z} \\
=-\frac{1}{\hat{y}}*a(1-a)\\
=-\frac{1}{\hat{y}}*\hat{y}(1-\hat{y})\\
=\hat{y}-1\\
=\hat{y}-y</script><p>其中 a = $\hat{y}$ ，$y=1$</p>
<hr>
<h4 id="3-10-深度学习框架"><a href="#3-10-深度学习框架" class="headerlink" title="3.10 深度学习框架"></a>3.10 深度学习框架</h4><p>易于编程，运行速度快，开源</p>
<hr>
<h4 id="3-11-TensorFlow"><a href="#3-11-TensorFlow" class="headerlink" title="3.11 TensorFlow"></a>3.11 TensorFlow</h4><p>例如cost function是参数w的函数：</p>
<script type="math/tex; mode=display">
\begin{equation}
 J=w^{2}-10 w+25 
\end{equation}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">coefficients = np.array([[<span class="number">1.</span>], [-<span class="number">10.</span>], [<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)  <span class="comment"># 定义参数w 初始化为0 </span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="number">3</span>,<span class="number">1</span>]) <span class="comment"># training data size为 3x1 稍后为x提供数值    现在x变成了控制这个二次函数系数的数据</span></span><br><span class="line">cost = x[<span class="number">0</span>][<span class="number">0</span>]*w**<span class="number">2</span> + x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># cost = tf.add(tf.add(w**2, tf.multiply(10., w)), 25) # 定义cost fucition</span></span><br><span class="line">cost = w**<span class="number">2</span> - <span class="number">10</span>*w + <span class="number">25</span> <span class="comment"># 重载了加减运算</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost) <span class="comment"># 优化器为梯度下降 学习率0.01 指定最小化函数为cost</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initalizer() <span class="comment"># 初始化</span></span><br><span class="line">session = tf.Sessions() <span class="comment"># 开启一个tf session </span></span><br><span class="line">session.run(init) <span class="comment"># 初始化全局变量 给w赋初值</span></span><br><span class="line">session.run(w) <span class="comment"># 评估变量w</span></span><br><span class="line"></span><br><span class="line">session.run(train) <span class="comment"># 开始优化 1步</span></span><br><span class="line">session.run(train, feed_dict = &#123;x:coefficients&#125;) <span class="comment"># 开始优化 并给x赋值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>): <span class="comment"># 优化1000步</span></span><br><span class="line">    session.run(train)</span><br><span class="line">    session.run(train, feed_dict = &#123;x:coefficients&#125;) <span class="comment"># 开始优化 并给x赋值</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><strong>TensorFlow</strong> 中的 <strong>placeholder</strong> 是一个你之后会赋值的变量，这种方式便于把训练数据加入损失方程，把数据加入损失方程用的是这个句法，当你运行训练迭代，用 <code>feed_dict</code> 来让 <code>x=coefficients</code>。</p>
<p>如果你在做 <strong>mini-batch</strong> 梯度下降，在每次迭代时，你需要插入不同的 <strong>mini-batch</strong>，那么每次迭代，你就用 <code>feed_dict</code> 来喂入训练集的不同子集，把不同的 <strong>mini-batch</strong> 喂入损失函数需要数据的地方。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">session = tf.Sessions() <span class="comment"># 开启一个tf session </span></span><br><span class="line">session.run(init) <span class="comment"># 初始化全局变量 给w赋初值</span></span><br><span class="line">session.run(w) <span class="comment"># 评估变量w</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以替换为</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session()  <span class="keyword">as</span> session:</span><br><span class="line">	session.run(init)</span><br><span class="line">    <span class="built_in">print</span>(session.run(w))</span><br></pre></td></tr></table></figure>
<p><strong>Python</strong>中的<strong>with</strong>命令更方便清理，以防在执行这个内循环时出现错误或例外</p>
<p>TensorFlow的最大优点就是采用数据流图（data flow graphs）来进行数值运算。图中的节点（Nodes）表示<strong>数学操作</strong>，图中的线（edges）则表示在节点间相互联系的<strong>多维数据数组</strong>，即张量（tensor）。而且它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。</p>
<hr>
<h2 id="第四周【人工智能行业大师访谈】"><a href="#第四周【人工智能行业大师访谈】" class="headerlink" title="第四周【人工智能行业大师访谈】"></a>第四周【人工智能行业大师访谈】</h2><h4 id="4-1-吴恩达采访-Yoshua-Bengio"><a href="#4-1-吴恩达采访-Yoshua-Bengio" class="headerlink" title="4.1. 吴恩达采访 Yoshua Bengio"></a>4.1. 吴恩达采访 Yoshua Bengio</h4><p>花书作者</p>
<p>self attention</p>
<p>but i dont think that we need that everything be formalized mathematically but be formalized logically, not the sense that i can convice somebody that this should be work, whether this make sense. This is the most important aspect. And then math allows us to make that stronger and tighter. </p>
<p>不认为一切事物都要数学化， 而是要逻辑化，并不是我可以让别人相信这样有用，可行。 然后再通过数学来强化和精练。</p>
<p>大多数人只停留在粗浅了解的程度，一旦出现问题，使用者很难解决，也不知道原因。所以大家要亲自实践，即便效率不高，只要知道是怎么回事就好，很有帮助，尽量亲自动手。 所以不要用那种几行代码就可以解决一切，却不知道其中原理的编程框架。尽量从基本原理入手获取知识。多阅读，多看别人的代码，多自己写代码。</p>
<p>不要畏惧数学，发展直觉认识，一旦在直觉经验层面得心应手，数学问题会变得更容易理解。 </p>
<hr>
<h4 id="4-2-吴恩达采访-林元庆"><a href="#4-2-吴恩达采访-林元庆" class="headerlink" title="4.2. 吴恩达采访 林元庆"></a>4.2. 吴恩达采访 林元庆</h4><p> 国家深度学习实验室</p>
<hr>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/03/28/C-DL-W1/" rel="next" title="C_DL_W1">
                <i class="fa fa-chevron-left"></i> C_DL_W1
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/03/28/C-DL-W3/" rel="prev" title="C_DL_W3">
                C_DL_W3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88"><span class="nav-number">1.</span> <span class="nav-text">深度学习工程师</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Question"><span class="nav-number">1.1.</span> <span class="nav-text">Question</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="nav-number">1.2.</span> <span class="nav-text">改善深层神经网络：超参数调试、正则化以及优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="nav-number">1.2.1.</span> <span class="nav-text">第一周 深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E8%AE%AD%E7%BB%83-%E9%AA%8C%E8%AF%81-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1.1 训练 &#x2F; 验证 &#x2F; 测试集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">1.2 偏差 &#x2F; 方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">1.3 机器学习基础</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">1.4 正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%AF%E4%BB%A5%E5%87%8F%E5%B0%91%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">1.5 为什么正则化可以减少过拟合？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-Dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.2.1.6.</span> <span class="nav-text">1.6 Dropout 正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-%E7%90%86%E8%A7%A3-Dropout"><span class="nav-number">1.2.1.7.</span> <span class="nav-text">1.7 理解 Dropout</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-8-%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.1.8.</span> <span class="nav-text">1.8 其他正则化方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-9-%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5"><span class="nav-number">1.2.1.9.</span> <span class="nav-text">1.9 归一化输入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-10-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">1.2.1.10.</span> <span class="nav-text">1.10 梯度消失与梯度爆炸</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-11-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.2.1.11.</span> <span class="nav-text">1.11 神经网络的权重初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-12-%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91"><span class="nav-number">1.2.1.12.</span> <span class="nav-text">1.12 梯度的数值逼近</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-13-%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="nav-number">1.2.1.13.</span> <span class="nav-text">1.13 梯度检验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-14-%E5%85%B3%E4%BA%8E%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%B3%A8%E8%AE%B0"><span class="nav-number">1.2.1.14.</span> <span class="nav-text">1.14 关于梯度检验实现的注记</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">第二周 优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.1 Mini-batch 梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E7%90%86%E8%A7%A3-mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.2 理解 mini-batch 梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">2.3 指数加权平均</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-%E7%90%86%E8%A7%A3%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">2.4 理解指数加权平均</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E7%9A%84%E5%81%8F%E5%B7%AE%E4%BF%AE%E6%AD%A3"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">2.5 指数加权平均的偏差修正</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">2.6 动量梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-RMSprop"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">2.7 RMSprop</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-8-Adam-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">2.8 Adam 优化算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-9-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="nav-number">1.2.2.9.</span> <span class="nav-text">2.9 学习率衰减</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-10-%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.2.10.</span> <span class="nav-text">2.10 局部最优的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch-%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6"><span class="nav-number">1.2.3.</span> <span class="nav-text">第三周 超参数调试、Batch 正则化和程序框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">3.1 调试处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E4%B8%BA%E8%B6%85%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">3.2 为超参数选择合适的范围</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%9APandas-VS-Caviar"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">3.3 超参数训练的实践：Pandas VS Caviar</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-%E5%BD%92%E4%B8%80%E5%8C%96%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">3.4 归一化网络的激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-%E5%B0%86-Batch-Norm-%E6%8B%9F%E5%90%88%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.3.5.</span> <span class="nav-text">3.5 将 Batch Norm 拟合进神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-Batch-Norm-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A5%8F%E6%95%88%EF%BC%9F"><span class="nav-number">1.2.3.6.</span> <span class="nav-text">3.6 Batch Norm 为什么奏效？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-7-%E6%B5%8B%E8%AF%95%E6%97%B6%E7%9A%84-Batch-Norm"><span class="nav-number">1.2.3.7.</span> <span class="nav-text">3.7 测试时的 Batch Norm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-Softmax-%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.3.8.</span> <span class="nav-text">3.8 Softmax 回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-9-%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA-Softmax-%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.2.3.9.</span> <span class="nav-text">3.9 训练一个 Softmax 分类器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-10-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-number">1.2.3.10.</span> <span class="nav-text">3.10 深度学习框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-TensorFlow"><span class="nav-number">1.2.3.11.</span> <span class="nav-text">3.11 TensorFlow</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E5%91%A8%E3%80%90%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%A1%8C%E4%B8%9A%E5%A4%A7%E5%B8%88%E8%AE%BF%E8%B0%88%E3%80%91"><span class="nav-number">1.3.</span> <span class="nav-text">第四周【人工智能行业大师访谈】</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E5%90%B4%E6%81%A9%E8%BE%BE%E9%87%87%E8%AE%BF-Yoshua-Bengio"><span class="nav-number">1.3.0.1.</span> <span class="nav-text">4.1. 吴恩达采访 Yoshua Bengio</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E5%90%B4%E6%81%A9%E8%BE%BE%E9%87%87%E8%AE%BF-%E6%9E%97%E5%85%83%E5%BA%86"><span class="nav-number">1.3.0.2.</span> <span class="nav-text">4.2. 吴恩达采访 林元庆</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Duan Yaqi</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>


<div class="powered-by">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
        本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">    
        本站访问量<span id="busuanzi_value_site_pv"></span>
    </span>
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
