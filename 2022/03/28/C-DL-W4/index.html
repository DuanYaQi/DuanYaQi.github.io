<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="DL," />










<meta name="description" content="深度学习工程师由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。">
<meta property="og:type" content="article">
<meta property="og:title" content="C_DL_W4">
<meta property="og:url" content="http://duanyaqi.com/2022/03/28/C-DL-W4/index.html">
<meta property="og:site_name" content="Duan&#39;s Blog">
<meta property="og:description" content="深度学习工程师由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MDkyMTM1NzA5">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MDkyODA1NTUz">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MTAwMzAwMDg1">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_b.webp">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210411155016785.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210411155020970.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjAwNDU1MjI2">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjAzODUzNTI2">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjMwNzA0Mjk3">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjMzMjQyNDk0">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTMwMTA1NTQ2MzIx">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI5MjAyMzIyMzA2">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210412151328057.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210412151343259.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI5MjAzODQxMzM2">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTMwMTAyNTMwNTQ0">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20200208104004691.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211150034018">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211155720094">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211175203203">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211204756960">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210414155426103.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211211417392">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211213835572">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171211215418919">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210414163056395.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212142205247">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212144647936">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212145859683">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212151353599">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212172342457">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171212175549666">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171213204922094">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171213211346970">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210416002203682.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210416000353405.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210416002825829.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210416002713830.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-617b082492f5c1c31bde1c6e2d994bc0_720w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-a20824492e3e8778a959ca3731dfeea3_720w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-2cdae9b3ad2f1d07e2c738331dac6d8b_720w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1503464-20191201220832846-1142021236.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618544228267.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618544247529.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618548406500.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618550446381.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618550796805.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618550842070.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618550852515.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618550954791.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171214102716526">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20171214104357412">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/6027faa79b81f9940281ea36ca901504.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180110222709201">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111160539785">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111162053588">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111164540407">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111165958113">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111195724054">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111203920946">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180111214333011">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-edbde717a2b81750e550f2ddb04ad81e_1440w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180112155405994">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180112162956443">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180112164507109">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419202712681.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180112174544272">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419214233973.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419214403342.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180112203741567">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419231140171.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419231437782.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419231923288.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419233112899.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419233152297.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210419233239505.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/854641-20180816102917864-126549672.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180109202649694">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1908255-20201029213626867-500143807.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1908255-20201029222848628-1109349561.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1908255-20201029223524056-20868724.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1908255-20201029224539185-1232935316.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/image-20210420010905023.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/edc4cabd5b84b3f4928743ce2673ab5d.webp">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180114153250005">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180114155747719">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180114174246276">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180114215601633">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115084732270">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115102626472">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115133731838">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115141321684">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1424e99481bed31de3f1c78a8bb544fbc81.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/eb60d01ca0925ee8445ee2d60e90ceda8b2.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/1618900252405.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115151648087">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115153352106">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115174238740">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115175808478">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/v2-27dcd5a25b5625cb92d97e02c12919c2_720w.jpg">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/899363-20171225164406634-1290119748.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/899363-20171225164426900-422413788.png">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115210830278">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115210923590">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180115211442581">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180305154333482">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180305180556590">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180305203908747">
<meta property="og:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/20180305212325555">
<meta property="article:published_time" content="2022-03-28T07:58:18.000Z">
<meta property="article:modified_time" content="2022-03-28T08:06:53.692Z">
<meta property="article:author" content="Duan Yaqi">
<meta property="article:tag" content="DL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://duanyaqi.com/2022/03/28/C-DL-W4/assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MDkyMTM1NzA5">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://duanyaqi.com/2022/03/28/C-DL-W4/"/>





  <title>C_DL_W4 | Duan's Blog</title>
  








<meta name="generator" content="Hexo 6.0.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a target="_blank" rel="noopener" href="https://github.com/DuanYaQi" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Duan's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">For AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://duanyaqi.com/2022/03/28/C-DL-W4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Duan's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">C_DL_W4</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-28T15:58:18+08:00">
                2022-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  20.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  77
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="深度学习工程师"><a href="#深度学习工程师" class="headerlink" title="深度学习工程师"></a>深度学习工程师</h1><p>由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。<br><span id="more"></span></p>
<p><strong>CNN可视化</strong></p>
<p><a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/">https://poloclub.github.io/cnn-explainer/</a></p>
<p>deeplearning.ai</p>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll">https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll</a></p>
<p><a target="_blank" rel="noopener" href="https://study.163.com/my#/smarts">https://study.163.com/my#/smarts</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66646276">https://www.bilibili.com/video/av66646276</a></p>
<p><strong>note</strong></p>
<p><a target="_blank" rel="noopener" href="https://redstonewill.blog.csdn.net/article/details/79055467">https://redstonewill.blog.csdn.net/article/details/79055467</a></p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/column/DeepLearningNotebook">https://www.zhihu.com/column/DeepLearningNotebook</a></p>
<p><strong>课后作业</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013733326/article/details/79827273">https://blog.csdn.net/u013733326/article/details/79827273</a></p>
<p><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/5e20243e2823a10036b542da">https://www.heywhale.com/mw/project/5e20243e2823a10036b542da</a></p>
<h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><ul>
<li>[ ] 特殊应用：人脸识别和神经风格转换-<a href="#4.7">4.7 CNN可视化解释</a>，伪影</li>
<li>[ ] 特殊应用：人脸识别和神经风格转换-<a href="#二维向量点积">4.10 style 损失函数-二维向量点积</a></li>
<li>[ ] 额外知识：白化</li>
</ul>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="第一周-卷积神经网络"><a href="#第一周-卷积神经网络" class="headerlink" title="第一周 卷积神经网络"></a>第一周 卷积神经网络</h3><h4 id="1-1-计算机视觉"><a href="#1-1-计算机视觉" class="headerlink" title="1.1 计算机视觉"></a>1.1 计算机视觉</h4><p>首先，计算机视觉的高速发展标志着新型应用产生的可能，这是几年前，人们所不敢想象的。通过学习使用这些工具，你也许能够创造出新的产品和应用。</p>
<p>其次，即使到头来你未能在计算机视觉上有所建树，也可以将<strong>所学的知识应用到其他算法和结构</strong>。</p>
<p>一张 64x64x3 的图片，神经网络输入层的维度为12288。一张 1000x1000x3 的图片，神经网络输入层的维度将达到 3M，使得网络权重 W 非常庞大。这样会造成两个后果，</p>
<ul>
<li>一是神经网络结构复杂，数据量相对不够，容易出现过拟合；</li>
<li>二是所需内存、计算量较大。</li>
</ul>
<p>解决这一问题的方法就是使用卷积神经网络（CNN）。</p>
<hr>
<h4 id="1-2-边缘检测示例"><a href="#1-2-边缘检测示例" class="headerlink" title="1.2 边缘检测示例"></a>1.2 边缘检测示例</h4><p>神经网络由浅层到深层，分别可以检测出图片的边缘特征 、局部特征（例如眼睛、鼻子等）、整体面部轮廓。</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MDkyMTM1NzA5" alt="这里写图片描述"></p>
<p>最常检测的图片边缘有两类：一是垂直边缘（vertical edges），二是水平边缘（horizontal edges）。</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MDkyODA1NTUz" alt="这里写图片描述"></p>
<p>图片的边缘检测可以通过与相应<strong>滤波器</strong>进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为6x6，滤波器filter尺寸为3x3，卷积后的图片尺寸为4x4，得到结果如下：</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MTAwMzAwMDg1" alt="这里写图片描述"></p>
<p>卷积过程动态示意图</p>
<p><img src="assets/v2-6428cf505ac1e9e1cf462e1ec8fe9a68_b.webp" alt="img"></p>
<hr>
<h4 id="1-3-更多边缘检测内容"><a href="#1-3-更多边缘检测内容" class="headerlink" title="1.3 更多边缘检测内容"></a>1.3 更多边缘检测内容</h4><p>还有很多其他的滤波器（检测算子）</p>
<p><img src="assets/image-20210411155016785.png" alt="image-20210411155016785"></p>
<p>随着深度学习的发展，我们学习的其中一件事就是当你真正想去检测出复杂图像的边缘，你不一定要去使用那些研究者们所选择的这九个数字，但你可以从中获益匪浅。把这矩阵中的9个数字当成9个参数，并且在之后你可以学习使用<strong>反向传播</strong>算法，其目标就是去<strong>理解这9个参数</strong>。</p>
<p><img src="assets/image-20210411155020970.png" alt="image-20210411155020970"></p>
<p>将这9个数字当成参数的思想，已经成为计算机视觉中最为有效的思想之一。</p>
<hr>
<h4 id="1-4-Padding"><a href="#1-4-Padding" class="headerlink" title="1.4 Padding"></a>1.4 Padding</h4><p><strong>本小节步长全部为1</strong></p>
<p>valid convolution : no padding</p>
<p>输入 <em> 卷积核 → 输出维度   （n, n） </em> （f, f） →  （n - f + 1, n - f + 1）</p>
<p>same convolution: padding   一搬填充为0</p>
<p>输入 <em> 卷积核 → 输出维度   （n + 2p, n + 2p） </em> （f, f） →  （n  + 2p - f + 1, n  + 2p - f + 1）</p>
<p>如果希望输出维度和原始输入维度一样，则计算得</p>
<script type="math/tex; mode=display">
p = \frac{f-1}{2}</script><p>这里也诠释了为什么卷积核一般为奇数尺寸。并且奇数有中心像素点，便于索引滤波器的位置。</p>
<p>odd number 奇数</p>
<p>even number 偶数</p>
<hr>
<h4 id="1-5-卷积步长"><a href="#1-5-卷积步长" class="headerlink" title="1.5 卷积步长"></a>1.5 卷积步长</h4><p>给定 padding : p 、strider: s</p>
<p>则有输入 <em> 卷积核 → 输出维度   （n, n） </em> （f, f） →  （$\frac{n+2p-f}{s}+1$, $\frac{n+2p-f}{s}+1$）</p>
<p>如果商不为整，向下取整 <code>floor</code>，有一部分超出范围就不进行计算。</p>
<p>相关系数（cross-correlations）与卷积（convolutions）之间是有区别的。真正的卷积运算（数学/信号处理）会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。filter旋转如下所示：</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjAwNDU1MjI2" alt="这里写图片描述"></p>
<p>而在深度学习领域，默认不需要反转，直接求积。严格意义来讲我们<strong>平时使用的方法不叫卷积而叫互相关</strong>。</p>
<p>之所以可以这么等效，是因为<strong>滤波器算子一般是水平或垂直对称的</strong>，180度旋转影响不大；而且最终滤波器算子需要通过CNN网络梯度下降算法计算得到，<strong>旋转部分可以看作是包含在CNN模型算法</strong>中。总的来说，忽略旋转运算可以大大提高CNN网络运算速度，而且不影响模型性能。</p>
<hr>
<h4 id="1-6-三维卷积"><a href="#1-6-三维卷积" class="headerlink" title="1.6 三维卷积"></a>1.6 三维卷积</h4><p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjAzODUzNTI2" alt="这里写图片描述"></p>
<p>对于3通道的RGB图片，其对应的滤波器算子同样也是3通道的。例如一个图片是6 x 6 x 3，分别表示图片的高度（height）、宽度（weight）和通道（channel）。</p>
<p>过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将<strong>3通道的和相加</strong>，得到输出图片的一个像素值。</p>
<p><strong>不同通道的滤波算子可以不相同</strong>。例如<strong>R</strong>通道filter实现<strong>垂直</strong>边缘检测，<strong>G和B</strong>通道<strong>不进行</strong>边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。</p>
<p>为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjMwNzA0Mjk3" alt="这里写图片描述"></p>
<p>则有输入 * 卷积核 → 输出维度   </p>
<script type="math/tex; mode=display">
（n, n, n_c） * n_k（f, f, n_c） →  （\frac{n+2p-f}{s}+1, \frac{n+2p-f}{s}+1, n_k）</script><p>$n$ 为图片尺寸大小，也可用 h 和 w 表示高和宽，例中为 6 ；</p>
<p>$f$ 为卷积核尺寸大小，例中为 3 ；</p>
<p>$n_c$ 为图片通道数目，例中为2；</p>
<p>$n_k$ 为滤波器组个数，例中为2；</p>
<p>padding : p 为填充数，例中为0，无填充；</p>
<p>strider: s 为步长，例中为1。则有：</p>
<script type="math/tex; mode=display">
（6, 6, 3） * 2（3, 3, 3） →  （4, 4, 2）</script><hr>
<h4 id="1-7-单层卷积网络"><a href="#1-7-单层卷积网络" class="headerlink" title="1.7 单层卷积网络"></a>1.7 单层卷积网络</h4><p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI4MjMzMjQyNDk0" alt="这里写图片描述"></p>
<p>相比之前的卷积过程，CNN 的单层结构多了激活函数 ReLU 和偏移量b。整个过程与标准的神经网络单层结构非常类似：</p>
<script type="math/tex; mode=display">
Z^{[l]}=W^{[l]} A^{[l-1]}+b\\
A^{[l]}=g^{[l]}\left(Z^{[l]}\right)</script><p>输出 = 非线性激活函数（线性函数+偏差），例中 W 为卷积核，A 为输入图像，b 为偏置，g为 <code>relu</code> 。相当于有两个个待学习的参数：<strong>卷积核，偏差</strong></p>
<p>总结CNN单层结构的所有标记符号，设层数为 $l$。</p>
<p><strong>filter size:</strong> $f^{[l]}$                    滤波器尺寸</p>
<p><strong>padding:</strong> $p^{[l]}$                       填充</p>
<p><strong>stride:</strong> $s^{[l]}$                            步长</p>
<p><strong>number of filters:</strong> $n_{c}^{[l]}$      滤波器个数</p>
<p><strong>input:</strong> $n_{H}^{[l-1]} \times n_{\omega}^{[l-1]} \times n_{c}^{[l-1]} $     输入维度，l-1层</p>
<p><strong>output:</strong>  $n_{H}^{[l]} \times n_{\omega}^{[l]} \times n_{c}^{[l]}$             输出维度，l层</p>
<p>其中  </p>
<script type="math/tex; mode=display">
\left.\begin{array}{l}
n_{H}^{[l]}=\left[\frac{n_{H}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right. \\
n_{W}^{[l]}=\left\lfloor\frac{n_{W}^{[l-1]}+2 p^{[l]}-f^{[l]}}{s^{[l]}}+1\right.
\end{array}\right]</script><p><strong>filter:</strong> $f^{[l]} \times f^{[l]} \times n_{c}^{[l-1] }$                           滤波器维度，最后一维与输入channel相同</p>
<p><strong>weights:</strong> $f^{[l]} \times f^{[l]} \times n_{c}^{[l-1]} \times n_{c}^{[l]}$           权重维度 = 滤波器维度 * 滤波器个数 </p>
<p><strong>bias:</strong> $1 \times 1 \times 1 \times n_{c}^{[l]}$                                偏置维度，只与滤波器个数有关</p>
<p><strong>activations:</strong> $n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}$                  激活函数维度，与输出维度完全相同</p>
<p>如果<strong>mini-batch</strong>有m个样本，进行向量化运算，相应的输出维度为 $m \times n_{H}^{[l]} \times n_{\omega}^{[l]} \times n_{c}^{[l]}$</p>
<p>假设你有10个过滤器，神经网络的一层是3×3×3，那么，这一层有多少个参数呢？</p>
<p>我们来计算一下，每一层都是一个3×3×3的矩阵，因此每个过滤器有27个参数，也就是27个数。然后加上一个偏差，用参数表示，现在参数增加到28个现在我们有10个过滤器，加在一起是28×10，也就是280个参数。</p>
<hr>
<h4 id="1-8-简单卷积网络示例"><a href="#1-8-简单卷积网络示例" class="headerlink" title="1.8 简单卷积网络示例"></a>1.8 简单卷积网络示例</h4><p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTMwMTA1NTQ2MzIx" alt="这里写图片描述"></p>
<p>CNN模型各层结构如上图所示。需要注意的是，$a^{[3]}$ 的维度是 <code>7 x 7 x 40</code>，将 $a^{[3]}$ 排列成 1 列，维度为 <code>1960 x 1</code>，然后连接最后一级输出层。输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出 $\hat y$ 。值得一提的是，随着CNN层数增加，$n_H^{[l]}$  和 $n_W^{[l]}$ 一般逐渐减小，而$n_c^{[l]}$ 一般逐渐增大。</p>
<p>CNN有三种类型的layer：</p>
<ul>
<li>Convolution层（CONV）</li>
<li>Pooling层（POOL）</li>
<li>Fully connected层（FC）</li>
</ul>
<hr>
<h4 id="1-9-池化层"><a href="#1-9-池化层" class="headerlink" title="1.9 池化层"></a>1.9 池化层</h4><p>缩减模型大小，提高计算速度，提高所提取特征的鲁棒性。</p>
<p><strong>最大池化</strong></p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI5MjAyMzIyMzA2" alt="这里写图片描述"></p>
<p>这就像是应用了一个规模为2的过滤器，因为我们选用的是2×2区域，步幅是2，这些就是最大池化的超参数。</p>
<p><img src="assets/image-20210412151328057.png" alt="image-20210412151328057"></p>
<p><img src="assets/image-20210412151343259.png" alt="image-20210412151343259"></p>
<p><strong>数字大意味着可能探测到了某些特定的特征</strong>，左上象限具有的特征可能是一个垂直边缘，一只眼睛。显然左上象限中存在这个特征，这个特征可能是一只猫眼探测器。必须承认，使用最大池化的主要原因是此方法在很多实验中效果都很好。计算卷积层输出大小的公式同样适用于最大池化，即 $\frac{n+2p-f}{s}+1$</p>
<p><strong>平均池化</strong></p>
<p>选取的不是每个过滤器的最大值，而是平均值。</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTI5MjAzODQxMzM2" alt="这里写图片描述"></p>
<p>目前来说，最大池化比平均池化更常用。但也有例外，就是深度很深的神经网络，可以用平均池化来分解规模为7×7×1000的网络的表示层，在整个空间内求平均值，得到1×1×1000。</p>
<p><strong>参数</strong></p>
<p>池化的超级参数包括过滤器大小 $f$ 和步幅 $s$ ,常用的参数值为 $f=2, s=2$，效果相当于高度和宽度缩减一半。很少用到超参数 padding。最常用的 padding 值是0，输入为$n_{H} \times n_{W} \times n_{c}$，输出为$ \lfloor\frac{n_{H}-f}{s}+1\rfloor \times\lfloor\frac{n_{\mathrm{w}}-f}{s}+ 1\rfloor \times n_{c}$。</p>
<p>输入与输出<strong>通道数相同</strong>，因为我们对每个通道都做了池化。需要注意的一点是，池化过程中<strong>没有需要学习的参数</strong>。执行反向传播时，反向传播没有参数适用于最大池化。这些设置过的超参数，可能是手动设置的，也可能是通过交叉验证设置的。</p>
<hr>
<h4 id="1-10-卷积神经网络示例"><a href="#1-10-卷积神经网络示例" class="headerlink" title="1.10 卷积神经网络示例"></a>1.10 卷积神经网络示例</h4><p>计算网络层数，通常是带有权重和参数的才算一层，想池化层和激活层就不算单独的一层。</p>
<p><img src="assets/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTcxMTMwMTAyNTMwNTQ0" alt="这里写图片描述"></p>
<p>图中，<code>CON</code> 层后面紧接一个 <code>POOL</code> 层，<code>CONV1</code> 和 <code>POOL1</code> 构成第一层，<code>CONV2</code> 和 <code>POOL2</code> 构成第二层。特别注意的是 <code>FC3</code> 和 <code>FC4</code> 为全连接层 <code>FC</code>，它跟标准的神经网络结构一致。最后的输出层（softmax）由10个神经元构成。</p>
<p>随着神经网络深度的加深，<strong>高度和宽度都会减小</strong>，从 32×32 到 28×28，到 14×14，到 10×10，再到 5×5；<strong>而通道数量会增加</strong>，从 3 到 6 到 16 不断增加，然后得到一个全连接层。</p>
<p>接下来我们讲讲神经网络的<strong>激活值形状</strong>，<strong>激活值大小</strong>和<strong>参数数量</strong>。有几点要注意，第一，池化层没有参数；第二，<strong>卷积层的参数相对较少</strong>，第三，<strong>全连接层参数较多</strong>，其实许多参数都存在于神经网络的全连接层。观察可发现，<strong>随着神经网络的加深，激活值size会逐渐变小，如果激活值size下降太快，也会影响神经网络性能</strong>。</p>
<p>整个网络各层的尺寸和参数如下表格所示：</p>
<p><img src="assets/20200208104004691.png" alt="在这里插入图片描述"></p>
<p><code>CONV1</code> 参数$ = 6 \times （5\times5\times3+1）$    //+1为bias</p>
<p><code>CONV2</code> 参数$ = 16 \times （5\times5\times6+1）$</p>
<p><code>FC2</code> 参数$ = （400\times120）+120$</p>
<p><code>FC3</code> 参数$ = （120\times84）+84$</p>
<p><code>softmax</code> 参数$ = （84\times10）+10$</p>
<p>上边的conv算法是每个通道滤波器做的事情不一样，如果每个通道滤波器一致，则参数量要除以通道数。</p>
<p>常规做法是，<strong>尽量不要自己设置超参数</strong>，而是<strong>查看文献中别人采用了哪些超参数</strong>，<strong>选一个在别人任务中效果很好的架构</strong>，那么它也有可能适用于你自己的应用程序。</p>
<hr>
<h4 id="1-11-为什么使用卷积？"><a href="#1-11-为什么使用卷积？" class="headerlink" title="1.11 为什么使用卷积？"></a>1.11 为什么使用卷积？</h4><p>与全连接层相比，卷积层的两个主要优势在于</p>
<p><strong>参数共享</strong>：一个特征检测器（例如垂直边缘检测）对图片A的某块区域有用，同时也可能作用在图片A的其它区域。即不用对图像不同区域使用不同的滤波器，所有区域只使用一个滤波器处理即可。（如果不参数共享，滤波器滑一次变一次，即下一步用新的参数滑，参数过多）</p>
<p><strong>稀疏连接</strong>：因为滤波器算子尺寸限制，每一层的每个输出只与输入<strong>部分区域</strong>内有关。而且其它像素值都不会对输出产生任影响。</p>
<p><strong>平移不变性</strong>：神经网络的卷积结构使得即使移动几个像素，这张图片依然具有非常相似的特征，应该属于同样的输出标记。</p>
<p>除此之外，由于 CNN 参数数目较小，所需的训练样本就相对较少，从而一定程度上不容易发生过拟合现象。而且，CNN比较擅长捕捉区域位置偏移。也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。</p>
<hr>
<h4 id="1-12-interview-yann-lecun"><a href="#1-12-interview-yann-lecun" class="headerlink" title="1.12 interview yann-lecun"></a>1.12 interview yann-lecun</h4><p>lenet</p>
<p>Conditional Random Field 条件随机场</p>
<p>给开源社区做贡献</p>
<hr>
<h3 id="第二周-深度卷积网络：实例探究"><a href="#第二周-深度卷积网络：实例探究" class="headerlink" title="第二周 深度卷积网络：实例探究"></a>第二周 深度卷积网络：实例探究</h3><h4 id="2-1-为什么要进行实例探究？"><a href="#2-1-为什么要进行实例探究？" class="headerlink" title="2.1 为什么要进行实例探究？"></a>2.1 为什么要进行实例探究？</h4><p>典型的CNN模型包括：</p>
<ul>
<li><strong>LeNet-5</strong>-1998 LeCun</li>
<li><strong>AlexNet</strong>-2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计</li>
<li><strong>VGG</strong>-2014年ILSVRC竞赛的第二名，第一名是GoogLeNet。</li>
</ul>
<p>除了这些性能良好的CNN模型之外，Residual Network（ResNet）的特点是可以构建很深很深的神经网络（目前最深的好像有152层）。Inception Neural Network。</p>
<hr>
<h4 id="2-2-经典网络"><a href="#2-2-经典网络" class="headerlink" title="2.2 经典网络"></a>2.2 经典网络</h4><p><strong>LeNet-5</strong></p>
<p><img src="assets/20171211150034018" alt="这里写图片描述"></p>
<p>5 层，6W 个参数，平均池化，sigmoid 和 tanh 函数，直接输出类别</p>
<blockquote>
<p>Gradient-Based Learning Applied to Document Recognition</p>
</blockquote>
<p><strong>AlexNet</strong></p>
<p><img src="assets/20171211155720094" alt="这里写图片描述"></p>
<p>60M 个参数，最大池化，relu 函数，局部响应归一化层 Local Response Normalization，softmax 输出各类别概率</p>
<blockquote>
<p>ImageNet Classification with Deep Convolutional Neural Networks</p>
</blockquote>
<p><strong>VGG-16</strong></p>
<p><img src="assets/20171211175203203" alt="这里写图片描述"></p>
<p>16 层，138M 个参数，只用 3x3 same 卷积，最大池化，专注于构建卷积层的简单网络，简化了神经网络架构</p>
<p>随着网络的加深，<strong>图像缩小的比例和通道数增加的比例是有规律的</strong>。很吸引人。</p>
<blockquote>
<p>very deep convolutional networks for large-scale image recognition</p>
<p>Visual Geometry Group Network - 视觉几何群网络</p>
</blockquote>
<hr>
<h4 id="2-3-残差网络"><a href="#2-3-残差网络" class="headerlink" title="2.3 残差网络"></a>2.3 残差网络</h4><p>Residual Networks (ResNets)</p>
<p>非常深的网络会出现梯度消失和梯度爆炸问题</p>
<p>skip connection / short cut</p>
<blockquote>
<p>Deep Residual Learning for Image Recognition</p>
</blockquote>
<p><strong>残差块</strong></p>
<p><img src="assets/20171211204756960" alt="这里写图片描述"></p>
<p>上图中红色部分就是skip connection，直接建立 $a[l]$ 与 $a[l+2]$ 之间的隔层联系。相应的表达式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
z^{[l+1]}=W^{[l+1]} a^{[l]}+b^{[l+1]}\\
a^{[l+1]}=g\left(z^{[l+1]}\right)\\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
a^{[l+2]}=g\left(z^{[l+2]}+a^{[l]}\right)
\end{equation}</script><p>$a[l]$ 直接隔层与下一层的线性输出相连，与 $z[l+2]$ 共同通过激活函数（ReLU）输出$a[l+2]$。</p>
<p><img src="assets/image-20210414155426103.png" alt="image-20210414155426103"></p>
<p>由多个 Residual block 组成的神经网络就是 Residual Network。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。Residual Network的结构如下图所示。</p>
<p><img src="assets/20171211211417392" alt="这里写图片描述"></p>
<p>另外，为了便于区分，我们把非 Residual Networks 称为 Plain Network。与 Plain Network 相比，Residual Network 能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸。从下面两张图的对比中可以看出，随着神经网络层数增加，Plain Network 实际性能会变差，training error 甚至会变大。然而，Residual Network 的训练效果却很好，training error 一直呈下降趋势。</p>
<p><img src="assets/20171211213835572" alt="这里写图片描述"></p>
<hr>
<h4 id="2-4-残差网络为什么有用？"><a href="#2-4-残差网络为什么有用？" class="headerlink" title="2.4 残差网络为什么有用？"></a>2.4 残差网络为什么有用？</h4><p>网络在训练集上表现好，才能在验证集和测试集上表现好。</p>
<p><img src="assets/20171211215418919" alt="这里写图片描述"></p>
<p>如上图所示，输入 $x$ 经过很多层神经网络后输出 $a^{[l]}$ ，$a^{[l]}$ 经过一个 Residual block 输出$a^{[l+2]}$。$a^{[l+2]}$ 的表达式为：</p>
<script type="math/tex; mode=display">
\begin{equation}a^{[l+2]}=g\left(z^{[l+2]}+a^{[l]}\right)=g\left(W^{[l+2]} a^{[l+1]}+b^{[l+2]}+a^{[l]}\right)\end{equation}</script><p>输入 $x$ 经过Big NN后，若 $W^{[l+2]}≈0$ ，$b^{[l+2]}≈0$，则有：</p>
<script type="math/tex; mode=display">
\begin{equation}a^{[l+2]}=g\left(a^{[l]}\right)=\operatorname{ReL} U\left(a^{[l]}\right)=a^{[l]} \quad when \quad a^{[l]} \geq 0\end{equation}</script><p>可以看出，即使发生了梯度消失，$W^{[l+2]}≈0$ ，$b^{[l+2]}≈0$，也能直接建立 $a^{[l+2]}$ 与 $a^{[l]}$ 的线性关系，且 $a^{[l+2]}=a^{[l]}$，这其实就是 Identity function。$a^{[l]}$ 直接连到 $a^{[l+2]}$，从效果来说，<strong>相当于直接忽略了$a^{[l]}$之后的这两层神经层</strong>。</p>
<p>这样，看似很深的神经网络，其实由于许多 Residual blocks 的存在，<strong>弱化削减了某些神经层之间的联系</strong>，实现<strong>隔层线性传递</strong>，而<strong>不是一味追求非线性关系</strong>，模型本身也就能“容忍”更深层的神经网络了。而且从性能上来说，这两层额外的 Residual blocks 也不会降低 Big NN的性能。当然，如果 Residual blocks <strong>确实能训练得到非线性关系</strong>，那么也<strong>会忽略 short cut</strong>，跟 Plain Network 起到同样的效果。</p>
<p>有一点需要注意的是，ResNet中使用了<strong>same卷积</strong>，使得$a^{[l]}$ 和 $a^{[l+2]}$ 的<strong>维度相同</strong>；但如果 Residual blocks 中 $a^{[l]}$ 和 $a^{[l+2]}$ 的<strong>维度不同</strong>，通常可以引入矩阵$W_s$，与 $a^{[l]}$ 相乘，使得 $W_s<em>a^{[l]}$ 的维度与 $a^{[l+2]}$ 一致。参数矩阵 $W_s$ 有来两种方法得到：一种是将 $W_s$ <strong>作为学习参数</strong>，通过模型训练得到；另一种是固定 $W_s$ 值（类似单位矩阵），不需要训练，$W_s$ 与 $a^{[l]}$ 的乘积仅仅使得 $a^{[l]}$ <em>*截断或者补零</em></em>。这两种方法都可行。</p>
<p><strong>ResNet 结构</strong><img src="assets/image-20210414163056395.png" alt="image-20210414163056395"></p>
<p>上图为普通的网络，输入image，多个卷积层，最后输出一个Softmax。只需要添加 skip connection，就转换为 ResNet，如下图：</p>
<p><img src="assets/20171212142205247" alt="这里写图片描述"></p>
<p>ResNets 同类型层之间，例如 CONV layers(实线)，大多使用 Same 类型，保持维度相同。如果是不同类型层之间的连接，例如 CONV layer 与 POOL layer 之间(虚线)，如果维度不同，则引入矩阵 $W_s$。</p>
<blockquote>
<p>$W^{[l+2]}$, $b^{[l+2]}$ 就相当于一个开关，只要让这两个参数 w 和 b 为 0，就可以达到增加网络深度却不影响网络性能的目的。而<strong>是否把这两个参数置为 0 就要看反向传播</strong>，网络最终能够知道到底要不要skip。</p>
<p>何凯明说过其实 AlexNet 是解决太深导致梯度消失，因为随着层数的增加计算出的梯度  会慢慢变小，可以映射到激活值会慢慢变小，之后他就想加一个 skip connection就可以保证无论中间多少层，最终两端激活值大体上不改变。</p>
<p>当网络不断加深时，就算是选用学习恒等函数的参数都很困难，所以很多层最后的表现不但没有更好，反而更糟。</p>
<p>这保证了深度的增加不会给模型带来负面影响，<strong>至少不会比 Plain Network 差</strong>。<strong>残差块很容易学习恒等映射</strong></p>
</blockquote>
<hr>
<h4 id="2-5-Network-in-network-以及-1×1-卷积"><a href="#2-5-Network-in-network-以及-1×1-卷积" class="headerlink" title="2.5 Network in network 以及 1×1 卷积"></a>2.5 Network in network 以及 1×1 卷积</h4><p>池化层可以压缩高度和宽度。1x1卷积可以压缩通道数。</p>
<p>1x1卷积，处理多通道的数据。即不同通道的数据加权求和，然后通过非线性函数。</p>
<p>对于单个 filter，1x1 的维度，意味着卷积操作等同于乘积操作。对于多个filters，1x1 Convolutions 的作用实际上<strong>类似全连接层</strong>的神经网络结构。效果等同于 Plain Network 中 $a^{[l]}$ 到 $a^{[l+1]}$ 的过程。</p>
<p><img src="assets/20171212144647936" alt="这里写图片描述"></p>
<p>1x1 Convolutions 可以用来缩减输入图片的通道数目:</p>
<p><img src="assets/20171212145859683" alt="这里写图片描述"></p>
<blockquote>
<p>Network in network, lin, 2013.</p>
</blockquote>
<hr>
<h4 id="2-6-Inception-network-motivation"><a href="#2-6-Inception-network-motivation" class="headerlink" title="2.6 Inception network motivation"></a>2.6 Inception network motivation</h4><p>创始，开端</p>
<p>构建卷积层时，你要决定过滤器的大小究竟是1×1，3×3 还是 5×5，或者要不要添加池化层。而Inception网络的作用就是<strong>代替你来决定</strong>，虽然网络架构因此变得更加复杂，但网络表现却非常好。<strong>代替人工来确定</strong>卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层。</p>
<p>Inception Network 在单层网络上可以使用多个<strong>不同尺寸</strong>的filters，进行<strong>same convolutions</strong>，把各filter下得到的输出<strong>拼接</strong>起来。除此之外，还可以将CONV layer与POOL layer<strong>混合</strong>，同时实现各种效果。但是要注意使用<strong>same pool</strong>。</p>
<p><img src="assets/20171212151353599" alt="这里写图片描述"></p>
<p>Inception Network在提升性能的同时，会带来计算量大的问题。例如：</p>
<p><img src="assets/20171212172342457" alt="这里写图片描述"></p>
<p>对于输出中的每个数字来说，都需要执行 <code>5×5×192</code> 次乘法运算，所以乘法运算的总次数为每个输出值所需要执行的乘法运算次数 <code>5×5×192</code> 乘以输出值个数 <code>28×28×32</code>，把这些数相乘结果等于 120 M。如果输出为 <code>28x28</code>，则使用一个 <code>5x5x192</code> 的卷积核，对原始输入做乘积计算即可，要做 <code>28x28</code> 次。</p>
<p>为此，我们可以引入1x1 Convolutions来减少其计算量，结构如下：</p>
<p><img src="assets/20171212175549666" alt="这里写图片描述"></p>
<p>通常我们把该1x1 Convolution称为“瓶颈层”（bottleneck layer）。引入bottleneck layer之后，总共需要的计算量为：<code>1x1X192 x 28x28x16</code> 和 <code>5x5x16 x 28x28x32</code> 相加为12.4 M。多引入了1x1 Convolution层，总共的计算量减少了近90%。由此可见，1x1 Convolutions 还可以有效减少 CONV layer 的计算量。</p>
<p>那么仅仅大幅<strong>缩小表示层规模</strong>会不会<strong>影响神经网络的性能</strong>？事实证明，只要<strong>合理构建瓶颈层</strong>，你既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算。这就是<strong>Inception</strong>模块的主要思想。</p>
<blockquote>
<p>Going deeper with convolutions. Szegedy et al. 2014.</p>
</blockquote>
<hr>
<h4 id="2-7-Inception-网络"><a href="#2-7-Inception-网络" class="headerlink" title="2.7 Inception 网络"></a>2.7 Inception 网络</h4><p>盗梦空间</p>
<p>引入1x1 Convolution 后的 <strong>Inception module</strong> 如下图所示：</p>
<p><img src="assets/20171213204922094" alt="这里写图片描述"></p>
<p><strong>多个</strong> Inception modules <strong>组成</strong> Inception Network，效果如下图所示：</p>
<p><img src="assets/20171213211346970" alt="这里写图片描述"></p>
<p><strong>个别的module中间添加了最大池化层来修改高和宽的维度</strong>，上述 Inception Network 除了由许多 Inception modules 组成之外，值得一提的是网络中间隐藏层也可以作为输出层Softmax，有利于防止发生过拟合。</p>
<hr>
<h4 id="2-8-MobileNet"><a href="#2-8-MobileNet" class="headerlink" title="2.8 MobileNet"></a>2.8 MobileNet</h4><p>可以在低计算环境下能够构建和部署正常工作的新网络。</p>
<p><strong>正常卷积</strong></p>
<p><img src="assets/image-20210416002203682.png" alt="image-20210416002203682"></p>
<p>计算代价 =  滤波器参数   x   输出像素size   x   滤波器个数</p>
<p>   2160     =   3 x 3 x 3      x          4 x 4          x        5  </p>
<p><strong>深度可分离卷积</strong> Depthwise-separable convolutions </p>
<p><img src="assets/image-20210416000353405.png" alt="image-20210416000353405"></p>
<p>Depthwise convolution</p>
<p><img src="assets/image-20210416002825829.png" alt="image-20210416002825829"></p>
<p>​          $n_{out} \times n_{out}\times n_{c}$                             $3 \times 3\times n_{c}$                               $n_{out} \times n_{out} \times n_{c}$</p>
<p>输出的 channel 与输入 channel 和滤波器的 channel 一样，滤波器的每个 channel，对应输出的每个 channel，即<strong>每个输入的 channel 只与滤波器的一个 channel 计算</strong>。</p>
<p>计算代价 =  滤波器参数   x   输出像素size   x   滤波器个数</p>
<p>​     432     =        3 x 3       x          4 x 4          x        3  </p>
<p>Pointwise convolution<img src="assets/image-20210416002713830.png" alt="image-20210416002713830"></p>
<p>   $n_{out} \times n_{out}\times n_{c}$                          $n^{\prime} \times  1 \times 1\times n_{c}$                            $n_{out} \times n_{out} \times n^{\prime}$</p>
<p>$n^{\prime}$ 为滤波器的个数</p>
<p>计算代价 =  滤波器参数   x   输出像素size   x   滤波器个数</p>
<p>​     240     =    1 x 1 x 3     x          4 x 4          x        5  </p>
<p>深度可分离卷积与正常卷积的成本比率为：</p>
<script type="math/tex; mode=display">
\frac{1}{n^{\prime}}+\frac{1}{f^2}</script><p>$n^{\prime}$ 为滤波器的个数，一般很大为64，128，256，512等</p>
<p>$f$ 为卷积核的尺寸，一般为 3</p>
<p>上例子中结果为 $\frac{1}{n^{\prime}}+\frac{1}{f^2}=\frac{1}{5}+\frac{1}{9}=\frac{432+240}{2160}$</p>
<p><strong>常规卷积</strong></p>
<p><img src="assets/v2-617b082492f5c1c31bde1c6e2d994bc0_720w.jpg" alt="img"></p>
<p><strong>深度可分离卷积</strong></p>
<ul>
<li>逐通道卷积</li>
</ul>
<p><img src="assets/v2-a20824492e3e8778a959ca3731dfeea3_720w.jpg" alt="img"></p>
<p><strong>无法扩展</strong> Feature map。而且这种运算对输入层的每个通道独立进行卷积运算，没有有效的利用<strong>不同通道在相同空间位置</strong>上的feature信息。</p>
<ul>
<li>逐点卷积</li>
</ul>
<p><img src="assets/v2-2cdae9b3ad2f1d07e2c738331dac6d8b_720w.jpg" alt="img"></p>
<p>将上一步的map在<strong>深度方向上进行加权组合</strong>，利用<strong>不同通道在相同空间位置</strong>上的feature信息，生成新的Feature map。</p>
<p>因此，在参数量相同的前提下，采用Separable Convolution的神经网络层数可以做的更深。</p>
<p><img src="assets/1503464-20191201220832846-1142021236.png" alt="img"></p>
<blockquote>
<p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. Howard et al. 2017.</p>
</blockquote>
<hr>
<h4 id="2-9-MobileNet-架构"><a href="#2-9-MobileNet-架构" class="headerlink" title="2.9 MobileNet 架构"></a>2.9 MobileNet 架构</h4><p><img src="assets/1618544228267.png" alt="1618544228267"></p>
<p>13 个 mobile block（浅蓝色虚线块）</p>
<p><img src="assets/1618544247529.png" alt="1618544247529"></p>
<ul>
<li>通过扩展操作 ，增加了 bottleneck中的表示形式，允许网络学习更丰富的功能。（<strong>解决了v1中无法扩大通道的问题</strong>） </li>
<li>通过depthwise和projection可以把输出的尺寸减小，减少存储这些值所需的内存量，以便传递到下一个块。</li>
<li>残差连接</li>
</ul>
<p>17 个 new mobile block（红色实线瓶颈块）</p>
<p><img src="assets/1618548406500.png" alt="1618548406500"></p>
<p>n x n x 3 → n x n x 18 → n x n x 18 → n x n x 3</p>
<p>expansion： 18个 1x1x3    一般为<strong>原始通道的6倍</strong>作为下一层的通道数</p>
<p>depthwise：18个 3x3x18   用padding保持原始尺寸大小</p>
<p>pointwise/projection：3个 1x1x18</p>
<p>如果做分类就最后池化层，全连接层，softmax 输出概率</p>
<blockquote>
<p>MobileNetV2: Inverted Residuals and Linear Bottlenecks. Sandler et al. 2019</p>
</blockquote>
<p>刚好与resnet的block相反</p>
<hr>
<h4 id="2-10-EfficientNet"><a href="#2-10-EfficientNet" class="headerlink" title="2.10 EfficientNet"></a>2.10 EfficientNet</h4><p>MobileNet V1 和 V2 提供了一种实现神经网络的方法，在计算上更有效。但是有没有办法调整 MobileNet 或其他架构到特定的设备。也许正在针对不同品牌的手机，不同数量的计算资源，或不同的边缘设备实施计算机视觉算法。</p>
<p>如果有更多的计算资源，则希望神经网络大，以便获得更高的准确性；如果在计算上受到更多限制，也许想要一个运行速度更快的神经网络，以一点点准确性为代价。如何为特定设备<strong>自动放大或缩小</strong>神经网络？</p>
<p><img src="assets/1618550446381.png" alt="1618550446381"></p>
<p>EfficientNet，为您提供了一种方法。假设您有一个基准神经网络架构，输入的图像具有一定的分辨率 resolution，而您的新网络具有一定的深度 depth，并且各层具有一定的宽度 width。</p>
<p>三个变量</p>
<ul>
<li>可以按比例放大或缩小图片，可以使用高分辨率图像。新的图像分辨率为 r。蓝色发光表示高分辨率的图像。</li>
</ul>
<p><img src="assets/1618550796805.png" alt="1618550796805"></p>
<ul>
<li>可以改变神经网络的深度 d，使该网络更深。</li>
</ul>
<p><img src="assets/1618550842070.png" alt="1618550842070"></p>
<ul>
<li>可以更改这些层的宽度 w，使层更宽。</li>
</ul>
<p><img src="assets/1618550852515.png" alt="1618550852515"></p>
<p>问题是，给定特定的计算资源，r，d 和 w 之间的<strong>最佳权衡</strong>是什么，以获得<strong>最佳性能</strong>在您的计算资源之内？分辨率提高10％，深度增加50％，和宽度增加20％？</p>
<p><img src="assets/1618550954791.png" alt="1618550954791"></p>
<p>如果想针对特定设备调整神经网络架构，EfficientNet 可以解决这一问题。</p>
<p>使用MobileNet，已经学会了如何构建计算效率更高的层，而使用EfficientNet，还可以找到一种方法，可以扩大或缩小这些神经网络</p>
<blockquote>
<p>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. Tan and Le, 2019.</p>
</blockquote>
<hr>
<h4 id="2-11-使用开源的实现方案"><a href="#2-11-使用开源的实现方案" class="headerlink" title="2.11 使用开源的实现方案"></a>2.11 使用开源的实现方案</h4><p>网络通常都需要很长的时间来训练，而或许有人已经使用多个<strong>GPU</strong>，通过庞大的数据集<strong>预先训练</strong>了这些网络，这样一来你就可以使用这些网络进行<strong>迁移学习</strong></p>
<hr>
<h4 id="2-12-迁移学习"><a href="#2-12-迁移学习" class="headerlink" title="2.12 迁移学习"></a>2.12 迁移学习</h4><p>你可以下载花费了别人好几周甚至几个月而做出来的开源的<strong>权重参数</strong>，把它当作一个<strong>很好的初始化</strong>用在你自己的神经网络上。用迁移学习把公共的数据集的知识迁移到你自己的问题上</p>
<p>假如说你要建立一个猫咪检测器，用来检测你自己的宠物猫。假如你的两只猫叫 Tigger 和 Misty，还有一种情况是，两者都不是。所以你现在有一个<strong>三分类问题</strong>。现在你可能没有Tigger 或者 Misty 的大量的图片，所以你的<strong>训练集会很小</strong>，你该怎么办呢？</p>
<p>建议你从网上下载一些神经网络开源的实现，把权重下载下来。有许多训练好的网络，你都可以下载。举个例子，<strong>ImageNet</strong>数据集，它有1000个不同的类别，因此这个网络会有一个<strong>Softmax</strong>单元，它可以输出1000个可能类别之一。</p>
<p>你可以去掉这个Softmax层，创建你<strong>自己的</strong>Softmax单元，用来输出 Tigger、Misty 和neither 三个类别。建议把<strong>所有的层看作是冻结的</strong>，<strong>冻结</strong>网络中所有层的<strong>参数</strong>，<strong>只需要训练和你的Softmax层有关的参数</strong>。这个<strong>Softmax</strong>层有三种可能的输出，Tigger<strong>、</strong>Misty或者都不是。</p>
<p>大多数深度学习框架也许会有<code>trainableParameter=0</code> 这样的参数，对于这些前面的层，你可能会设置这个参数。有时也会有<code>freeze=1</code> 这样的参数。在这个例子中，你<strong>只需要训练softmax层的权重，把前面这些层的权重都冻结</strong>。</p>
<p>另一个技巧，由于前面的层都冻结了，相当于一个<strong>固定的函数</strong>。取输入图像，然后把它映射到这层（<strong>softmax</strong>的前一层）的激活函数。所以这个能加速训练的技巧就是，如果我们先计算这一层，计算特征或者激活值，然后把它们存到硬盘里。对你的计算有用的一步就是对你的训练集中所有样本的这一层的激活值进行<strong>预计算</strong>，然后存储到硬盘里，然后在此之上训练<strong>softmax</strong>分类器。</p>
<p>因此如果你的任务只有一个很小的数据集，你可以这样做。要有一个<strong>更大的训练集</strong>怎么办呢？也许你有大量的 Tigger 和 Misty 的照片，还有两者都不是的，这种情况，你应该<strong>冻结更少的层</strong>。取后面几层的权重，用作<strong>初始化</strong>，然后从这里开始梯度下降。或者你可以<strong>直接去掉</strong>这几层，换成你自己的隐藏单元和你自己的<strong>softmax</strong>输出层但是有一个规律，</p>
<p>如果你有越来越多的数据，你需要冻结的层数越少，你能够训练的层数就越多。</p>
<p>最后，如果你有大量数据，你应该做的就是用开源的网络和它的权重，把所有的权重当作<strong>初始化</strong>，然后<strong>训练整个网络</strong>。再次注意，如果这是一个1000节点的<strong>softmax</strong>，而你只有三个输出，你需要你自己的<strong>softmax</strong>输出层来输出你要的标签。</p>
<p>如果你有越多的标定的数据，或者越多的<strong>Tigger</strong>、<strong>Misty</strong>或者两者都不是的图片，你可以训练<strong>更多的层</strong>。极端情况下，你可以用下载的权重只作为初始化，用它们来代替随机初始化，接着你可以用梯度下降训练，更新网络所有层的所有权重。</p>
<p>这就是卷积网络训练中的迁移学习，事实上，网上的公开数据集非常庞大，并且你下载的其他人已经训练好几周的权重，已经从数据中学习了很多了，你会发现，<strong>对于很多计算机视觉的应用，如果你下载其他人的开源的权重，并用作你问题的初始化，你会做的更好</strong>。总之，迁移学习是非常值得你考虑的，除非你有一个极其大的数据集和非常大的计算量预算来从头训练你的网络。</p>
<hr>
<h4 id="2-13-数据增强"><a href="#2-13-数据增强" class="headerlink" title="2.13 数据增强"></a>2.13 数据增强</h4><p>提高性能</p>
<p>无论是使用迁移学习用别人的预训练模型开始，或者从源代码开始训练模型，数据扩充会有帮助。</p>
<p>常用的Data Augmentation方法是对已有的样本集进行Mirroring和Random Cropping。垂直镜像对称，随机裁剪。</p>
<p>旋转，剪切（<strong>shearing</strong>：此处并非裁剪的含义，图像仅水平或垂直坐标发生变化）图像，扭曲变形等等。这些方法并没有坏处，太复杂所以使用很少。</p>
<p><img src="assets/20171214102716526" alt="这里写图片描述"></p>
<p>另一种Data Augmentation的方法是color shifting。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片<strong>色调</strong>。对<strong>R</strong>、<strong>G</strong> 和 <strong>B</strong> 的值是根据某种概率分布来决定的</p>
<p><img src="assets/20171214104357412" alt="这里写图片描述"></p>
<p>针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，<strong>对主要的通道颜色进行增加或减少</strong>，可以采用<strong>高斯扰动</strong>。这样也能增加有效的样本数量。可以查阅AlexNet的相关论文。<strong>算法对照片的颜色更改更具鲁棒性。</strong></p>
<hr>
<h4 id="2-14-计算机视觉现状"><a href="#2-14-计算机视觉现状" class="headerlink" title="2.14 计算机视觉现状"></a>2.14 计算机视觉现状</h4><p><strong>Benchmark</strong> 基准测试，<strong>Benchmark</strong>是一个评价方式，在整个计算机领域有着长期的应用。Benchmark在计算机领域应用最成功的就是性能测试，主要测试负载的执行时间、传输速度、吞吐量、资源占用率等。</p>
<blockquote>
<p>Wiki ：“As computer architecture advanced, it became more difficult to compare the performance of various computer systems simply by looking at their specifications.Therefore, tests were developed that allowed comparison of different architectures.”</p>
</blockquote>
<ul>
<li><p>集成</p>
<p>可以独立训练几个神经网络，并平均它们的输出。比如说随机初始化三个、五个或者七个神经网络，然后训练所有这些网络，然后平均它们的输出。</p>
</li>
<li><p><strong>Multi-crop at test time</strong></p>
<p>将数据扩充应用到你的测试图像。<strong>10-crop</strong></p>
<p><img src="assets/6027faa79b81f9940281ea36ca901504.png" alt="img"></p>
</li>
<li><p>灵活使用开源代码：</p>
<p>Use <strong>archittectures of networks</strong> published in the literature</p>
<p>Use open source <strong>implementations</strong> if possible</p>
<p>Use <strong>pretrained models</strong> and <strong>fine-tune</strong> on your dataset</p>
</li>
</ul>
<hr>
<h3 id="第三周-目标检测"><a href="#第三周-目标检测" class="headerlink" title="第三周 目标检测"></a>第三周 目标检测</h3><h4 id="3-1-目标定位"><a href="#3-1-目标定位" class="headerlink" title="3.1 目标定位"></a>3.1 目标定位</h4><p>boudingbox 参数表示bx, by, bh, bw</p>
<p><img src="assets/20180110222709201" alt="这里写图片描述"></p>
<p>输出的label可表示为：</p>
<script type="math/tex; mode=display">
\left [  
\begin{matrix}
Pc \\
bx \\
by \\
bh \\
bw \\
c1 \\
c2 \\
c3
\end{matrix}  
\right ]</script><p>Pc=1，表示有目标，即 $y_1=1$：</p>
<script type="math/tex; mode=display">
L(\hat y,y)=(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+\cdots+(\hat y_8-y_8)^2</script><p>Pc=0，表示无目标，其余7个参数没有意义，即 $y_1=0$：</p>
<script type="math/tex; mode=display">
L(\hat y,y)=(\hat y_1-y_1)^2</script><p>当然，除了使用平方误差之外，还可以逻辑回归损失函数，类标签 $c_1,c_2,c_3$ 也可以通过softmax输出。平方误差已经能够取得比较好的效果。</p>
<hr>
<h4 id="3-2-特征点检测"><a href="#3-2-特征点检测" class="headerlink" title="3.2 特征点检测"></a>3.2 特征点检测</h4><p>landmark</p>
<p>人脸识别/骨架提取/姿态提取。输出特征点坐标。<strong>训练集标签里也要有特征点的准确位置</strong>。</p>
<p><img src="assets/20180111160539785" alt="这里写图片描述"></p>
<p><img src="assets/20180111162053588" alt="这里写图片描述"></p>
<hr>
<h4 id="3-3-目标检测"><a href="#3-3-目标检测" class="headerlink" title="3.3 目标检测"></a>3.3 目标检测</h4><p>滑动窗口法。首先在训练样本集上搜集相应的各种目标图片和非目标图片。注意<strong>训练集图片尺寸较小，尽量仅包含相应目标</strong>，如下图所示：</p>
<p><img src="assets/20180111164540407" alt="这里写图片描述"></p>
<p>然后，使用这些训练集构建CNN模型，使得模型有较高的识别率。</p>
<p>最后，在测试图片上，选择<strong>大小适宜的窗口</strong>、<strong>合适的步进长度</strong>，进行从左到右、从上倒下的滑动。每个窗口区域都送入之前构建好的 CNN 模型进行识别判断。若判断有目标，则此窗口即为目标区域；若判断没有目标，则此窗口为非目标区域。</p>
<p><img src="assets/20180111165958113" alt="这里写图片描述"></p>
<p>滑动窗口：窗口大，精度低，计算成本低；窗口小，精度高，计算成本高。</p>
<hr>
<h4 id="3-4-卷积的滑动窗口实现"><a href="#3-4-卷积的滑动窗口实现" class="headerlink" title="3.4 卷积的滑动窗口实现"></a>3.4 卷积的滑动窗口实现</h4><p>Convolutional implementation of sliding windows</p>
<p>滑动窗算法可以使用卷积方式实现，以提高运行速度，节约重复运算成本。滑动窗口算法卷积实现的第一步就是<strong>将全连接层转变成为卷积层</strong>，如下图所示：</p>
<p><img src="assets/20180111195724054" alt="这里写图片描述"></p>
<p>全连接层转变成卷积层的操作很简单，只需要使用与上层尺寸一致的滤波算子进行卷积运算即可。最终得到的输出层维度是1 x 1 x 4，代表4类输出值。</p>
<p>单个窗口区域卷积网络结构建立完毕之后，对于待检测图片，即可使用该网络参数和结构进行运算。例如16 x 16 x 3的图片，<strong>步进长度为2</strong>（因为用到了一次 2*2 的 max pool，所以滑动窗口的步长也是 2），CNN网络得到的输出层为2 x 2 x 4。其中，2 x 2表示共有4个窗口结果。对于更复杂的28 x 28 x3的图片，CNN网络得到的输出层为8 x 8 x 4，共64个窗口结果。</p>
<p><img src="assets/20180111203920946" alt="这里写图片描述"></p>
<p>滑动窗算法需要反复进行CNN正向计算，例如16 x 16 x 3的图片需进行4次，28 x 28 x3的图片需进行64次。而利用卷积操作代替滑动窗算法，则不管原始图片有多大，只需要进行一次CNN正向计算，因为其中<strong>共享了很多重复计算部分</strong>，这大大节约了运算成本。<strong>窗口步进长度与选择的MAX POOL大小有关</strong>。如果需要步进长度为4，只需设置MAX POOL为4 x 4即可。</p>
<blockquote>
<p>OverFeat: Integrated recognition, localization and detection using convolutional networks. Sermanet et al, 2014.</p>
</blockquote>
<hr>
<h4 id="3-5-Bounding-Box-预测"><a href="#3-5-Bounding-Box-预测" class="headerlink" title="3.5 Bounding Box 预测"></a>3.5 Bounding Box 预测</h4><p>相当于小节 3.5 = 3.1+ 3.4</p>
<p>上一小节，提高了速度，但是不能输出最精准的bounding box。YOLO（You Only Look Once）算法可以解决这类问题，生成更加准确的目标区域（如上图红色窗口）。</p>
<p>YOLO算法首先将原始图片分割成n x n网格，每个网格代表一块区域。为简化说明，下图中将图片分成3 x 3网格。</p>
<p><img src="assets/20180111214333011" alt="这里写图片描述"></p>
<p>然后，利用上一节卷积形式实现滑动窗口算法的思想，对该原始图片构建CNN网络，得到的输出层维度为 3 x 3 x 8。其中，3 x 3 对应9个网格，每个网格的输出包含8个元素：</p>
<script type="math/tex; mode=display">
y=\left [
\begin{matrix}
Pc \\
bx \\
by \\
bh \\
bw \\
c1 \\
c2 \\
c3
\end{matrix}
\right ]</script><p>如果目标中心坐标 $(b_x,b_y)$ 不在当前网格内，则当前网格 $P_c=0$；相反，则当前网格 $P_c=1$（即只看中心坐标是否在当前网格内）。判断有目标的网格中，$b_x,b_y,b_h,b_w$ 限定了目标区域。值得注意的是，当前网格左上角坐标设定为 $(0, 0)$，右下角坐标设定为 $(1, 1)$，$(b_x,b_y)$ 范围限定在 $[0,1]$ 之间，但是 $b_h,b_w$ 可以大于1。因为目标可能超出该网格，横跨多个区域。目标占几个网格没有关系，目标中心坐标必然在一个网格之内。</p>
<p><img src="assets/v2-edbde717a2b81750e550f2ddb04ad81e_1440w.jpg" alt="anchor boxes 如何起作用"></p>
<p>划分的网格可以更密一些。网格越小，则多个目标的中心坐标被划分到一个网格内的概率就越小，这恰恰是我们希望看到的。</p>
<blockquote>
<p>You Only Look Once: Unified real-time object detection. Redmon et al, 2015.</p>
</blockquote>
<hr>
<h4 id="3-6-交并比"><a href="#3-6-交并比" class="headerlink" title="3.6 交并比"></a>3.6 交并比</h4><p><strong>Intersection over union</strong></p>
<p>判断目标检测算法性能IoU，计算两个边界框合并集之比，一般大于0.5 即可通过。</p>
<p>IOU衡量了两个边界框重叠的相对大小，判断两个边界框是否相似。</p>
<p><img src="assets/20180112155405994" alt="这里写图片描述"></p>
<p>如上图所示，红色方框为真实目标区域，蓝色方框为检测目标区域。两块区域的交集为绿色部分，并集为紫色部分。蓝色方框与红色方框的接近程度可以用 IoU 比值来定义：</p>
<script type="math/tex; mode=display">
IoU = \frac{I}{U}</script><p>IoU 可以表示任意两块区域的接近程度。IoU 值介于 0～1 之间，且越接近 1 表示两块区域越接近。</p>
<hr>
<h4 id="3-7-非极大值抑制"><a href="#3-7-非极大值抑制" class="headerlink" title="3.7 非极大值抑制"></a>3.7 非极大值抑制</h4><p>Non-max suppression：解决一个物体，多次被检测。输出概率最大的结果。 </p>
<p>YOLO算法中，可能会出现多个网格都检测出到同一目标的情况，例如几个相邻网格都判断出同一目标的中心坐标在其内。</p>
<p><img src="assets/20180112162956443" alt="这里写图片描述"></p>
<p>上图中，三个绿色网格和三个红色网格分别检测的都是同一目标。那如何判断哪个网格最为准确呢？方法是使用非最大值抑制算法。</p>
<p>图示中每个网格的 $P_c$ 值可以求出，$P_c$ 值反映了该网格包含目标中心坐标的可信度。首先选取  $P_c$  最大值对应的网格和区域，然后计算该区域与所有其它区域的IoU，剔除掉IoU大于阈值（例如0.5）的所有网格及区域。这样就能保证同一目标只有一个网格与之对应，且该网格Pc最大，最可信。接着，再从剩下的网格中选取 $P_c$ 最大的网格，重复上一步的操作。最后，就能使得每个目标都仅由一个网格和区域对应。如下图所示：</p>
<p><img src="assets/20180112164507109" alt="这里写图片描述"></p>
<p>总结一下非最大值抑制算法的流程：</p>
<ul>
<li><strong>1. 剔除Pc值小于某阈值（例如0.6）的所有网格；</strong></li>
<li><strong>2. 选取Pc值最大的网格，利用网格中的bounding box信息计算IoU，摒弃与该网格交叠较大的网格；</strong></li>
<li><strong>3. 对剩下的网格，重复步骤2。</strong></li>
</ul>
<p>删掉其余框有两个条件：Pc较低并且IoU相对高</p>
<p>上边的例子Pc是概率表示仅表示目标是车的概率，如果尝试同时检测三个对象，比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次</p>
<hr>
<h4 id="3-8-Anchor-Boxes"><a href="#3-8-Anchor-Boxes" class="headerlink" title="3.8 Anchor Boxes"></a>3.8 Anchor Boxes</h4><p>每个格子只能检测出一个目标。对于多个目标重叠的情况，应使用不同形状的Anchor Boxes。</p>
<p>如下图所示，同一网格出现了两个目标：人和车。为了同时检测两个目标，我们可以设置两个Anchor Boxes，Anchor box 1检测人，Anchor box 2检测车。也就是说，每个网格多加了一层输出。原来的输出维度是 3 x 3 x 8，现在是3 x 3 x 2 x 8（也可以写成3 x 3 x 16的形式）。这里的2表示有两个Anchor Boxes，用来在一个网格中同时检测多个目标。每个Anchor box都有一个Pc值，若两个Pc值均大于某阈值，则检测到了两个目标。<br><img src="assets/image-20210419202712681.png" alt="image-20210419202712681"></p>
<p>现在<strong>每个对象都和之前一样分配到同一个格子中</strong>，即对象中心所在的格子。同时也需要<strong>分配到和目标形状 IoU 最高的 Anchor Box</strong>。例如有两个 Anchor Box 而单元格中只剩一个对象，则选取IoU高的，另一个的输出 $P_c$ 则为0。与第一个box的IoU高，则把信息填在向量前半部分：</p>
<script type="math/tex; mode=display">
y=\left [
\begin{matrix}
P_c \quad
b_x \quad
b_y \quad
b_h \quad
b_w \quad
c_1 \quad
c_2 \quad
c_3 \quad
P_c \quad
b_x \quad
b_y \quad
b_h \quad
b_w \quad
c_1 \quad
c_2 \quad
c_3
\end{matrix}
\right ]^T</script><p>使用YOLO算法时，只需对每个Anchor box使用上一节的非最大值抑制即可。Anchor Boxes之间并行实现。</p>
<p>目前anchor box的选择主要有三种方式：</p>
<ul>
<li>人为经验选取</li>
<li>k-means聚类</li>
<li>作为超参数进行学习</li>
</ul>
<p><strong>为什么不直接将bounding box参数加入loss进行训练？</strong></p>
<p>首先注意 yolo v1 就是这样做的，即 anchor-free，直接预测具体的boundng box参数。至于yolo v2/v3使用anchor-box的原因如下：</p>
<ul>
<li><p>误差对大的box和小的box有一样的权重，但是对于大的box和小的box是不能按一样的速度进行更新的，较小的误差对于大的box就没有对小的box那么重要。</p>
</li>
<li><p><strong>降低学习难度。</strong>anchor给出了目标宽高的初始值，需要回归的是目标真实<strong>宽高</strong>与初始宽高的<strong>偏移量</strong>，而不使用anchor的做法需要回归宽高的<strong>绝对量</strong>。</p>
</li>
</ul>
<hr>
<h4 id="3-9-YOLO-算法"><a href="#3-9-YOLO-算法" class="headerlink" title="3.9 YOLO 算法"></a>3.9 YOLO 算法</h4><p><img src="assets/20180112174544272" alt="这里写图片描述"></p>
<p>输入图片，通过网络预测，得到一个张量。得到所有 $p_c$ 不为 0 的bounding box。如下图：</p>
<p><img src="assets/image-20210419214233973.png" alt="image-20210419214233973"></p>
<p>通过非极大值抑制。</p>
<ul>
<li><strong>1. 剔除Pc值小于某阈值（例如0.6）的所有网格；</strong>（下图左→下图中）</li>
<li><strong>2. 选取Pc值最大的网格，利用网格中的bounding box信息计算IoU，摒弃与该网格交叠较大的网格；</strong>（下图中→下图右，就是把概率最大的邻居bounding box删除掉）</li>
<li><strong>3. 对剩下的网格，重复步骤2。</strong></li>
</ul>
<p><img src="assets/image-20210419214403342.png" alt="image-20210419214403342"></p>
<hr>
<h4 id="3-10-R-CNN"><a href="#3-10-R-CNN" class="headerlink" title="3.10 R-CNN"></a>3.10 R-CNN</h4><p>之前的算法在显然没有任何目标的区域仍进行计算。因此提出 Region proposals 候选区域。具体做法是先对原始图片进行<strong>分割</strong>算法处理，然后对分割后的图片中的块进行目标检测。</p>
<p><img src="assets/20180112203741567" alt="这里写图片描述"></p>
<p>R-CNN（Regions with Convolutional Neural Network Features）。Region Proposals 共有三种方法：</p>
<ul>
<li>R-CNN:  求候选区域，然后运行分类网络。输入区域-输出标签和bounding box（anchor-free）。</li>
<li>Fast R-CNN: 利用卷积实现滑动窗算法，类似3.4小节。</li>
<li>Faster R-CNN: 利用<strong>卷积</strong>对图片进行<strong>分割</strong>（segmentation）RPN，进一步提高运行速度。</li>
</ul>
<p>比较而言，Faster R-CNN的运行速度还是比YOLO慢一些。首先得到候选区域，然后再分类。</p>
<p>候选区域/框 + 深度学习分类：通过提取候选区域，并对相应区域进行以深度学习方法为主的分类的方案：</p>
<p>R-CNN（Selective Search + CNN + SVM）</p>
<p>Fast R-CNN（Selective Search + CNN + ROI）</p>
<p>Faster R-CNN（RPN + CNN + ROI）</p>
<p>基于深度学习的回归方法：YOLO/SSD/DenseBox 等方法</p>
<blockquote>
<p>Selective Search &gt; 根据颜色，边缘，纹理找可能存在的目标候选框。</p>
<p>ROI Pooling &gt; 将proposal抠出来的过程，然后resize到统一的大小。</p>
<p>RPN &gt; Region Proposal Network</p>
<p>SPPNet &gt; 空间金字塔池化实现任意大小图像输入。</p>
<p>Rich feature hierarchies for accurate object detection and semantic segmentation. Girshik et al, 2013.</p>
<p>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. Kaiming He.</p>
</blockquote>
<hr>
<h4 id="3-11-语义分割U-Net"><a href="#3-11-语义分割U-Net" class="headerlink" title="3.11 语义分割U-Net"></a>3.11 语义分割U-Net</h4><p>语义分割，目的是绘制在检测到的物体周围绘制详细的轮廓，以便您确切地知道哪个像素属于目标，哪些像素不属于目标。</p>
<p>什么是语义分割？</p>
<p>如果您想让学习算法找出这张图片中的每个像素是什么，那么您可以使用语义分割算法其目标是输出最右图。</p>
<p><img src="assets/image-20210419231140171.png" alt="image-20210419231140171"></p>
<p>通过语义分割，算法尝试标记道路的每个像素，都以深绿色表示。自动驾驶团队使用它来准确找出哪些像素可以安全行驶。</p>
<p>让我们看看其他一些应用程序。在左图的胸部 X 光检查图像中，使用不同的颜色分割出肺部，心脏和锁骨。右图脑MRI扫描自动分割出肿瘤。</p>
<p><img src="assets/image-20210419231437782.png" alt="image-20210419231437782"></p>
<p>为了简单起见，让我们使用以下示例从某些背景中细分出一辆汽车。在此图像中 1 代表汽车，2 代表建筑物，3 代表道路。</p>
<p><img src="assets/image-20210419231923288.png" alt="image-20210419231923288"></p>
<p>传统分类问题</p>
<p><img src="assets/image-20210419233112899.png" alt="image-20210419233112899"></p>
<p>转变为语义分割问题</p>
<p><img src="assets/image-20210419233152297.png" alt="image-20210419233152297"></p>
<p>语义分割的一个关键步骤是将<strong>越来越小</strong>的图像尺寸<strong>逐渐放大</strong>回完整的输入图像尺寸。</p>
<p><img src="assets/image-20210419233239505.png" alt="image-20210419233239505"></p>
<p>随着深度增加，变小的 size 将变大，而增大的通道数将减少，最终得到了猫的分割图。</p>
<hr>
<h4 id="3-12-转置卷积"><a href="#3-12-转置卷积" class="headerlink" title="3.12 转置卷积"></a>3.12 转置卷积</h4><p>transpose convolution</p>
<p>从信息论的角度看，卷积是<strong>不可逆的</strong>。所以这里说的并不是从 output 矩阵和 kernel 矩阵计算出原始的 input 矩阵。而是计算出一个<strong>保持了位置性关系</strong>的矩阵。</p>
<p><img src="assets/854641-20180816102917864-126549672.png" alt="img"></p>
<p>重叠的部分相加。转置卷积，它所进行的操作本质上就是先对输入进行 padding，然后再进行卷积得到输出。</p>
<p>input 为 2x2，filter 为 3x3，result 为 5x5。padding = 1，stride = 2</p>
<p><img src="assets/20180109202649694" alt="img"></p>
<p>转换为矩阵的形式是由卷积的结果得到的，矩阵形式本身是不能直接获得的。要注意这个因果关系，转换为矩阵形式是为了便于理解，以及推导转置卷积。</p>
<p>kernel_size = 2, stride = 1, padding = 0</p>
<p><img src="assets/1908255-20201029213626867-500143807.png" alt="img"></p>
<p>kernel_size = 2, stride = 1, padding = 1</p>
<p><img src="assets/1908255-20201029222848628-1109349561.png" alt="img"></p>
<p>与上一张图的主要不同之处在于转置卷积将卷积结果的最外层去掉，这是因为padding=1，也正符合与卷积相反的操作。也就是说，padding越大，转置卷积就会去掉越多的外层，输出就会越小。</p>
<p>kernel_size = 3, stride = 1, padding = 1</p>
<p><img src="assets/1908255-20201029223524056-20868724.png" alt="img"></p>
<p>kernel_size = 2, stride = 2, padding = 1</p>
<p><img src="assets/1908255-20201029224539185-1232935316.png" alt="img"></p>
<blockquote>
<p>直接理解转置卷积（Transposed convolution）的各种情况 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/qizhou/p/13895967.html">https://www.cnblogs.com/qizhou/p/13895967.html</a></p>
</blockquote>
<hr>
<h4 id="3-13-U-Net架构直觉"><a href="#3-13-U-Net架构直觉" class="headerlink" title="3.13 U-Net架构直觉"></a>3.13 U-Net架构直觉</h4><p>正卷积用于网络的前半部分，对图像进行压缩。图片尺寸变小，深度很大，丢失了很多空间信息。后半部分使用转置卷积放大到原始输入图像的 size（低分辨率和高水平的特征信息）。因此通过skip-connection使神经网络获取（高分辨率和低水平的特征信息)</p>
<hr>
<h4 id="3-14-U-Net架构"><a href="#3-14-U-Net架构" class="headerlink" title="3.14 U-Net架构"></a>3.14 U-Net架构</h4><p><img src="assets/image-20210420010905023.png" alt="image-20210420010905023"></p>
<p>输入为 h x w x 3，conv 增加 channel，maxpool 减小size。trans conv 增加 size，减少 channel （浅蓝色为转置卷积输出）。注意每次转置卷积后，与 skip-connection 拼接后<strong>要过几次正向卷积</strong>。输出为 h x w x nc（nc为分类数目）。采用argmax，将每个像素分类为其中一个类别。</p>
<p><strong>损失函数</strong>用了 pixel-wise softmax，就是每个像素对应的输出单独做 softmax，也就是做了 h * w 个 softmax。其中，x 可以看作是某一个像素点， $l(x)$ 表示 x 这个点对应的类别 label，$p_k(x)$ 表示在 x 这个点的输出在类别 k 的 softmax 的值，$p_{l(x)}(x)$ 代表点 x 在对应的 label 给出的那个类别的输出的值。</p>
<p><img src="assets/edc4cabd5b84b3f4928743ce2673ab5d.webp" alt="img"></p>
<blockquote>
<p>Ｕ-Net: Convolutional Networks for Biomedical Image Segmentation. Ronneberger et al, 2015.</p>
</blockquote>
<hr>
<h3 id="第四周-特殊应用：人脸识别和神经风格转换"><a href="#第四周-特殊应用：人脸识别和神经风格转换" class="headerlink" title="第四周 特殊应用：人脸识别和神经风格转换"></a>第四周 特殊应用：人脸识别和神经风格转换</h3><h4 id="4-1-什么是人脸识别？"><a href="#4-1-什么是人脸识别？" class="headerlink" title="4.1 什么是人脸识别？"></a>4.1 什么是人脸识别？</h4><p>介绍一下人脸验证（face verification）和人脸识别（face recognition）的区别。</p>
<ul>
<li>人脸验证：输入一张人脸图片，验证输出与模板是否为同一人，即一对一问题。</li>
<li>人脸识别：输入一张人脸图片，验证输出是否为 K 个模板中的某一个，即一对多问题。</li>
</ul>
<p>一般地，人脸识别比人脸验证更难一些。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K 个模板都进行比较，则相应的错误率就会增加，约 K %。模板个数越多，错误率越大一些。</p>
<hr>
<h4 id="4-2-One-Shot-学习"><a href="#4-2-One-Shot-学习" class="headerlink" title="4.2 One-Shot 学习"></a>4.2 One-Shot 学习</h4><p>One-shot learning 就是说数据库中每个人的训练样本只包含一张照片，然后训练一个 CNN 模型来进行人脸识别。若数据库有 K 个人，则CNN模型输出 softmax 层就是 K 维的。但是 One-shot learning 的性能并不好，其包含了两个缺点：</p>
<p>每个人只有一张图片，训练样本少，构建的 CNN 网络不够健壮。</p>
<p>若数据库增加另一个人，输出层 softmax 的维度就要发生变化，相当于要重新构建 CNN 网络，使模型计算量大大增加，不够灵活。</p>
<p>为了解决 One-shot learning 的问题，我们先来介绍相似函数（similarity function）。相似函数表示两张图片的相似程度，用 $d(img1,img2)$ 来表示。若 $d(img1,img2)$ 较小，则表示两张图片相似；若 $d(img1,img2)$ 较大，则表示两张图片不是同一个人。相似函数可以在人脸验证中使用：</p>
<ul>
<li>$d(img1,img2)\leq \tau$ : 一样</li>
<li>$d(img1,img2)&gt; \tau$ : 不一样</li>
</ul>
<p>对于人脸识别问题，则只需计算测试图片与数据库中 K 个目标的相似函数，取其中  $d(img1,img2)$  最小的目标为匹配对象。若所有的  $d(img1,img2)$  都很大，则表示数据库没有这个人。</p>
<p><img src="assets/20180114153250005" alt="这里写图片描述"></p>
<hr>
<h4 id="4-3-Siamese-网络"><a href="#4-3-Siamese-网络" class="headerlink" title="4.3 Siamese 网络"></a>4.3 Siamese 网络</h4><p>若一张图片经过一般的 CNN 网络（包括 CONV 层、POOL 层、FC 层），最终得到全连接层 FC，该 FC 层可以看成是原始图片的编码 encoding，表征了原始图片的关键特征。这个网络结构我们称之为 Siamese network。也就是说每张图片经过 Siamese network 后，由 FC 层每个神经元来表征。</p>
<p><img src="assets/20180114155747719" alt="这里写图片描述"></p>
<p>建立Siamese network后，两张图片 $x^{(1)}$ 和 $x^{(2)}$ 的相似度函数可由各自FC层 $f(x^{(1)})$ 与 $f(x^{(2)})$ 之差的范数来表示：</p>
<script type="math/tex; mode=display">
d(x^{(1)},x^{(2)})=||f(x^{(1)})-f(x^{(2)})||^2</script><p>值得一提的是，不同图片的 CNN 网络所有结构和参数都是一样的。我们的目标就是利用梯度下降算法，不断调整网络参数，使得属于同一人的图片之间 $d(x^{(1)},x^{(2)})$ 很小，而不同人的图片之间 $d(x^{(1)},x^{(2)})$ 很大。</p>
<p>若 $x^{(i)},x^{(j)}$ 是同一个人，则 $||f(x^{(i)})-f(x^{(j)})||^2$ 较小</p>
<p>若 $x^{(i)},x^{(j)}$ 不是同一个人，则 $||f(x^{(i)})-f(x^{(j)})||^2$ 较大</p>
<blockquote>
<p>DeepFace closing the gap to human level performance. Taigman et al, 2014.</p>
</blockquote>
<hr>
<h4 id="4-4-Triplet-损失"><a href="#4-4-Triplet-损失" class="headerlink" title="4.4 Triplet 损失"></a>4.4 Triplet 损失</h4><p>margin  软间隔</p>
<p>Triplet Loss 需要每个样本包含三张图片：靶目标（Anchor）、正例（Positive）、反例（Negative），这就是triplet 名称的由来。顾名思义，靶目标和正例是同一人，靶目标和反例不是同一人。Anchor 和 Positive 组成一类样本，Anchor 和 Negative 组成另外一类样本。</p>
<p><img src="assets/20180114174246276" alt="这里写图片描述"></p>
<p>我们希望上一小节构建的CNN网络输出编码 $f(A)$ 接近  $f(P)$ ，即 $||f(A)-f(P)||^2$ 尽可能小，而 $||f(A)-f(N)||^2$ 尽可能大，数学上满足：</p>
<script type="math/tex; mode=display">
||f(A)-f(P)||^2\leq ||f(A)-F(N)||^2\\
||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq 0</script><p>根据上面的不等式，如果所有的图片都是零向量，即 $f(A)=0,f(P)=0,f(N)=0$，那么上述不等式也满足。但是这对我们进行人脸识别没有任何作用，是不希望看到的。我们希望得到 $||f(A)-f(P)||^2$ 远小于$||f(A)-f(N)||^2$ 。所以，我们添加一个超参数 $α$，且 $α&gt;0$，对上述不等式做出如下修改：</p>
<script type="math/tex; mode=display">
||f(A)-f(P)||^2-||f(A)-F(N)||^2\leq -\alpha\\
||f(A)-f(P)||^2-||f(A)-F(N)||^2+\alpha \leq 0</script><p>这里的 $α$ 也被称为边界 margin，类似与支持向量机中的 margin。接下来，我们根据A，P，N三张图片，就可以定义Loss function为：</p>
<script type="math/tex; mode=display">
L(A,P,N)=max(||f(A)-f(P)||^2-||f(A)-F(N)||^2+\alpha,\ 0)</script><p>相应地，对于m组训练样本，cost function为：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^mL(A^{(i)},P^{(i)},N^{(i)})</script><p>关于训练样本，必须保证同一人包含多张照片，否则无法使用这种方法。例如 10k 张照片包含 1k 个不同的人脸，则平均一个人包含 10 张照片。这个训练样本是满足要求的。然后，就可以使用梯度下降算法，不断训练优化 CNN 网络参数，让 $J$ 不断减小接近0。</p>
<p>同一组训练样本，A，P，N 的选择尽可能不要使用随机选取方法。因为随机选择的 A 与 P 一般比较接近，A 与 N 相差也较大，毕竟是两个不同人脸。这样的话，也许模型不需要经过复杂训练就能实现这种明显识别，但是抓不住关键区别。所以，最好的做法是人为选择 <strong>A 与 P 相差较大</strong>（例如换发型，留胡须等），<strong>A 与 N 相差较小</strong>（例如发型一致，肤色一致等）。这种人为地<strong>增加难度和混淆度</strong>会让模型本身去寻找学习不同人脸之间关键的差异，“尽力”让 $d(A,P)$ 更小，让 $d(A,N)$ 更大，即<strong>让模型性能更好</strong>。</p>
<p><img src="assets/20180114215601633" alt="这里写图片描述"></p>
<blockquote>
<p>FaceNet: A unified embedding for face recognition and clustering. Schroff et al, 2015.</p>
</blockquote>
<hr>
<h4 id="4-5-面部验证与二分类"><a href="#4-5-面部验证与二分类" class="headerlink" title="4.5 面部验证与二分类"></a>4.5 面部验证与二分类</h4><p>除了构造 triplet loss 来解决人脸识别问题之外，还可以使用二分类结构。做法是将两个 siamese 网络组合在一起，将各自的编码层输出经过一个逻辑输出单元，该神经元使用 sigmoid 函数，输出 1 则表示识别为同一人，输出 0 则表示识别为不同人。结构如下：</p>
<p><img src="assets/20180115084732270" alt="这里写图片描述"></p>
<p>每组训练样本包含两张图片，每个 siamese 网络结构和参数完全相同。这样就把人脸识别问题转化成了一个二分类问题。引入逻辑输出层参数 w 和 b，输出 $\hat{y}$ 表达式为：</p>
<script type="math/tex; mode=display">
\hat y=\sigma(\sum_{k=1}^Kw_k|f(x^{(i)})_k-f(x^{(j)})_k|+b)</script><p>其中参数 $w_k$ 和 $b$ 都是通过梯度下降算法迭代训练得到。$\hat{y}$ 的另外一种表达式为：</p>
<script type="math/tex; mode=display">
\hat y=\sigma(\sum_{k=1}^Kw_k\frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}+b)</script><p>上式被称为 $χ$ 方公式，也叫 $χ$ 方相似度。</p>
<p>在训练好网络之后，进行人脸识别的常规方法是测试图片与模板分别进行网络计算，编码层输出比较，计算逻辑输出单元。</p>
<p>为了<strong>减少计算量</strong>，可以使用<strong>预计算</strong>的方式在训练时就将数据库每个模板的<strong>编码层输出</strong> $f(x)$ <strong>保存</strong>下来。因为编码层输出 $f(x)$ 比原始图片数据量少很多，所以无须保存模板图片，只要保存每个模板的 $f(x)$ 即可，节约存储空间。而且，测试过程中，无须计算模板的 siamese 网络，只要计算测试图片的 siamese 网络，得到的 $f(x^{(i)})$ 直接与存储的模板 $f(x^{(j)})$ 进行下一步的逻辑输出单元计算即可，计算时间减小了接近一半。这种方法也可以应用在上一节的 triplet loss 网络中。</p>
<hr>
<h4 id="4-6-什么是神经风格转换？"><a href="#4-6-什么是神经风格转换？" class="headerlink" title="4.6 什么是神经风格转换？"></a>4.6 什么是神经风格转换？</h4><p>它可以实现将一张图片的风格“迁移”到另外一张图片中，生成具有其特色的图片。</p>
<p><img src="assets/20180115102626472" alt="这里写图片描述"></p>
<p>一般用 C 表示内容图片，S 表示风格图片，G 表示生成的图片。</p>
<hr>
<h4 id="4-7-深度卷积网络在学什么？"><a href="#4-7-深度卷积网络在学什么？" class="headerlink" title="4.7 深度卷积网络在学什么？"></a>4.7 深度卷积网络在学什么？<span id ="4.7"></span></h4><p>典型的CNN网络如下所示：</p>
<p><img src="assets/20180115133731838" alt="这里写图片描述"></p>
<p>首先来看第一层隐藏层，<strong>遍历所有训练样本</strong>，<strong>针对每个卷据核</strong>，都找出让该层<strong>激活函数输出最大的9块图像区域</strong>；得到9 x 9的图像如下所示，其中每个3 x 3区域表示一个运算单元。左上角红框是通过第一个隐藏单元，并激活后输出最大的 9 副图像。第一层一共 9 个隐藏单元，另外 8 个如下。</p>
<p><img src="assets/20180115141321684" alt="这里写图片描述"></p>
<p>可以看出，第一层隐藏层一般检测的是原始图像的<strong>边缘和颜色阴影</strong>等简单信息。继续看CNN的更深隐藏层，随着层数的增加，捕捉的区域更大，特征更加复杂，从边缘到纹理再到具体物体。</p>
<p><img src="assets/1424e99481bed31de3f1c78a8bb544fbc81.png" alt="img"></p>
<p>层1展示了物体边缘、颜色等图像中底层的特征</p>
<p>层2展示了物体的边缘和轮廓，以及与颜色的组合；</p>
<p>层3拥有了更复杂的不变性，展示了相似的纹理，网格；花纹；</p>
<p>层4开始体现类与类之间的差异，其中不同组别的重构特征之间存在重大差异性；狗脸，鸟腿</p>
<p>层5展示了存在重大差异的一类物体；键盘，狗。</p>
<p>本文通过反卷积可视化</p>
<p><img src="assets/eb60d01ca0925ee8445ee2d60e90ceda8b2.png" alt="img"></p>
<p>从右边卷积的过程开始，首先用卷积核 F 对上一层池化出来的 Pooled Maps 进行卷积，得到 Feature Maps，然后在逐步进行 Relu 归一化 (Rectified Linear) 和最大值池化（Max Pooling）。</p>
<p>而反卷积过程则是从反最大值池化开始（Max unpooling），逐步得到unpooled、rectified unpooled和reconstruction map。</p>
<p><strong>反卷积神经网络主要组成部分：</strong></p>
<ul>
<li>反池化：</li>
</ul>
<p>对于最大值池化来说，它是不可逆的过程，因此作者的技巧就是在池化的时<strong>候记录下每个最大值的位置</strong>。这样的话，在反池化的时候只要把池化过程中最大激活值所在的位置激活，<strong>其它位置的值赋0</strong>。</p>
<ul>
<li>反激活：</li>
</ul>
<p>对于 Relu 激活函数来，激活值均为非负值。因此对于反向过程，同样需要保证每层的特征值为非负值，因此Reluctant <strong>反激活过程和激活过程相同</strong>。</p>
<ul>
<li>反卷积：</li>
</ul>
<p>卷积网络就是网络利用学习到的卷积核对上一层的特征进行卷积得到本层的 feature map。而反卷积就是这个过程的逆过程，用本层的 feature map 与<strong>转置后的卷积核</strong>进行卷积，得到上一层的特征。就是转置卷积。</p>
<p><strong>可视化网络如何提升网络性能？</strong></p>
<p>作者可视化了原版 AlexNet 各特征层，发现了对于 AlexNet 来说，第一层（图b）的卷积核大部分是高频和低频的特征，而对中频段图像特征整提取得不好。同时，第二层特征的可视化的结果（图d）显示出了由于第一层卷积步长 4 太大导致的 “混叠伪影”(aliasing artifacts)。因此作者对 AlexNet 的改善包括：将第一层的卷积核从 11x11 减小为 7x7（图c)；将卷积步长减小为 2 (图e）。经过作者改善后的模型在第一层和第二层特征中保留了更多的信息，提高了分类性能。</p>
<p><img src="assets/1618900252405.png" alt="1618900252405"></p>
<blockquote>
<p>Visualizing and understanding convolutional networks. Zeiler and Fergus., 2013.</p>
</blockquote>
<hr>
<h4 id="4-8-代价函数"><a href="#4-8-代价函数" class="headerlink" title="4.8 代价函数"></a>4.8 代价函数</h4><p>神经风格迁移生成图片 $G$ 的cost function由两部分组成：$C$ 与 $G$ 的相似程度和 $S$ 与 $G$ 的相似程度。</p>
<script type="math/tex; mode=display">
J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G)</script><p>其中，$α, β$ 是超参数，用来调整 $J_{content}(C,G)$ 与 $J_{style}(S,G)$ 的相对比重。</p>
<p><img src="assets/20180115151648087" alt="这里写图片描述"></p>
<p>神经风格迁移的基本算法流程是：</p>
<p>首先令 $G：100\times 100\times 3$ 为随机像素点，</p>
<p>然后使用梯度下降算法 $G = G - \frac{\part}{\part G}J(G)$，不断修正 $G$ 的所有像素点，使得 $J(G)$ 不断减小，从而使 $G$ 逐渐有 $C$ 的内容和 $G$ 的风格，如下图所示。</p>
<p><img src="assets/20180115153352106" alt="这里写图片描述"></p>
<blockquote>
<p>A neural algorithm of artistic style. Gatys et al, 2015. </p>
<p>Image on slide generated by Justin Johnson.</p>
</blockquote>
<hr>
<h4 id="4-9-content-代价函数"><a href="#4-9-content-代价函数" class="headerlink" title="4.9 content 代价函数"></a>4.9 content 代价函数</h4><p>我们先来看 $J(G)$ 的第一部分 $J_{content}(C,G)$，它表示内容图片 $C$ 与生成图片 $G$ 之间的相似度。</p>
<script type="math/tex; mode=display">
J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G)</script><p>使用预训练好的的 CNN 网络（AlexNet，VGG等）。$C，S，G$ <strong>共用相同模型和参数</strong>。首先，需要选择合适的层数 $l$ 来计算 $J_{content}(C,G)$。根据小节 4.7 的内容，CNN 的每个隐藏层分别提取原始图片的不同深度特征，由简单到复杂。如果 $l$ 太小，则 $G$ 与 $C$ 在像素上会非常接近，没有迁移效果；如果 $l$ 太深，则 $G$ 上某个区域将直接会出现 $C$ 中的物体。因此，$l$ 既不能太浅也不能太深，一般选择网络中间层。</p>
<p>然后比较 $C$ 和 $G$ 在 $l$ 层的激活函数输出 $a^{<a href="C">l</a>}$ 与 $a^{<a href="G">l</a>}$。相应的 $J_{content}(C,G)$ 的表达式为(L2范数)：</p>
<script type="math/tex; mode=display">
J_{content}(C,G)=\frac12||a^{[l](C)}-a^{[l](G)}||^2</script><p> $a^{<a href="C">l</a>}$ 与 $a^{<a href="G">l</a>}$越相似，则说明两个图片的内容相似，则 $J_{content}(C,G)$ 越小。$\frac{1}{2}$ 为归一化操作，可以通过总 cost 函数的超参数 $\alpha$ 进行学习。</p>
<p>方法就是使用梯度下降算法，不断迭代修正 $G$ 的像素值，使 $J_{content}(C,G)$ 不断减小。</p>
<hr>
<h4 id="4-10-style-损失函数"><a href="#4-10-style-损失函数" class="headerlink" title="4.10 style 损失函数"></a>4.10 style 损失函数</h4><p>我们来看 $J(G)$ 的第二部分 $J_{style}(S,G)$，它表示内容图片 $S$ 与生成图片 $G$ 之间的相似度。</p>
<script type="math/tex; mode=display">
J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G)</script><p>什么是图片的风格？利用CNN网络模型，图片的风格可以定义成第 $l$ 层隐藏层不同通道间激活函数的乘积（相关性）。</p>
<p><img src="assets/20180115174238740" alt="这里写图片描述"></p>
<p>例如我们选取第 $l$ 层隐藏层，其各通道使用不同颜色标注，如下图所示。因为每个通道提取图片的特征不同，比如 1 通道（红色）提取的是图片的垂直纹理特征，2 通道（黄色）提取的是图片的橙色背景特征。那么计算这两个通道的相关性大小，相关性越大，表示原始图片及既包含了垂直纹理也包含了该橙色背景；相关性越小，表示原始图片并没有同时包含这两个特征。也就是说，计算不同通道的相关性，反映了原始图片特征间的相互关系，从某种程度上刻画了图片的“风格”。</p>
<p><img src="assets/20180115175808478" alt="这里写图片描述"></p>
<p>接下来我们就可以定义图片的风格矩阵（style matrix）为：</p>
<script type="math/tex; mode=display">
G_{kk'}^{[l]}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{ijk}^{[l]}a_{ijk'}^{[l]}</script><p>其中，$a$ 表示 $k$ 通道上 $[i,j]$ 位置的激活值。$[l]$ 表示第 $l$ 层隐藏层，$k，k’$ 分别表示不同通道，总共通道数为 $n_C^{[l]}$。$i，j$ 分别表示该隐藏层的高度和宽度。如果两个 $a$ 都大，相乘也大，对应的 $G$ 也大，相关性高。</p>
<p>风格矩阵 $G_{kk’}^{[l]}$ 计算第 $l$ 层隐藏层不同通道对应的所有激活函数输出和。$G_{kk’}^{[l]}$ 的维度为 $n_c^{[l]} \times n_c^{[l]}$。若<strong>两个通道之间相似性高</strong>，则对应的 $G_{kk’}^{[l]}$ <strong>较大</strong>；若<strong>两个通道之间相似性低</strong>，则对应的  $G_{kk’}^{[l]}$  <strong>较小</strong>。</p>
<p>风格矩阵 $G_{kk’}^{<a href="S">l</a>}$ 表征了风格图片 $S$ 第 $l$ 层隐藏层的“风格”。相应地，生成图片 $G$ 也有 $G_{kk’}^{<a href="G">l</a>}$。那么，$G_{kk’}^{<a href="S">l</a>}$ 与 $G_{kk’}^{<a href="G">l</a>}$ 越相近，则表示 $G$ 的风格越接近 $S$。这样，我们就可以定义出的 $J^{[l]}_{style}(S,G)$ 表达式：</p>
<script type="math/tex; mode=display">
J^{[l]}_{style}(S,G)=\frac{1}{(2n_H^{[l]}n_W^{[l]}n_C^{[l]})}\sum_{k=1}^{n_C^{[l]}}\sum_{k'=1}^{n_C^{[l]}}||G_{kk'}^{[l][S]}-G_{kk'}^{[l][G]}||_F^2</script><p>其中 $\frac{1}{(2n_H^{[l]}n_W^{[l]}n_C^{[l]})}$ 为归一化常数，定义完 $J^{[l]}_{style}(S,G)$ 之后，我们的目标就是使用梯度下降算法，不断迭代修正 $G$ 的像素值，使$J^{[l]}_{style}(S,G)$ 不断减小。<strong>Frobenius</strong>范数，实际上是计算两个矩阵对应元素相减的平方的和。</p>
<p>以上我们只比较计算了一层隐藏层 $l$ 。为了提取的“风格”更多，使用多层隐藏层，然后相加：</p>
<script type="math/tex; mode=display">
J_{style}(S,G)=\sum_l\lambda^{[l]}\cdot J^{[l]}_{style}(S,G)</script><p>其中，$λ^{[l]}$ 表示累加过程中各层 $J^{[l]}_{style}(S,G)$ 的权重系数，为超参数。根据以上两小节的推导，最终的 cost function 为：</p>
<script type="math/tex; mode=display">
J(G)=\alpha \cdot J_{content}(C,G)+\beta \cdot J_{style}(S,G)</script><p>使用梯度下降算法进行迭代优化。</p>
<p>即先对<strong>风格图</strong>和<strong>生成图</strong>的每一层滤波 feature map 分别求 gram 矩阵，再求其距离的平方和，再将 5 层的结果加权求和。</p>
<p><img src="assets/v2-27dcd5a25b5625cb92d97e02c12919c2_720w.jpg" alt="img"></p>
<p><strong>什么是 gram 矩阵，为啥要用 gram 矩阵？</strong></p>
<p>Gram 矩阵就是每一层滤波后的 feature map, 后将其转置并相乘得到的矩阵，如下图所示。其实就是<strong>不同滤波器滤波结果 feature map 两两之间的相关性</strong>。譬如说，（如下图）某一层中有一个滤波器专门检测尖尖的塔顶这样的东西，另一个滤波器专门检测黑色。又有一个滤波器负责检测圆圆的东西，又有一个滤波器用来检测金黄色。</p>
<p><img src="assets/899363-20171225164406634-1290119748.png" alt="img"></p>
<p><img src="assets/899363-20171225164426900-422413788.png" alt="img"></p>
<p>对梵高的原图做Gram矩阵，谁的相关性会比较大呢？如上图所示，“尖尖的”和“黑色”总是一起出现的，它们的相关性比较高。而“圆圆的”和“金黄色”都是一起出现的，他们的相关性比较高。因此在风格转移的时候，其实也在风景图里去寻找这种“匹配”，<strong>将尖尖的渲染为黑色</strong>，<strong>将圆圆的渲染为金黄色</strong>。如果我们承认“<strong>图像的艺术风格就是其基本形状与色彩的组合方式</strong>” ，这样一个假设，那么Gram矩阵能够表征艺术风格就是理所当然的事情了。</p>
<p><strong>为什么G大相关性就大？</strong><span id = "二维向量点积"></span></p>
<p>从向量点乘角度有助于理解格拉姆矩阵。向量点乘可以看作衡量两个向量的相似程度，对于二维向量来说，两个单位向量，方向一致，点乘为1，相互垂直，点乘为0，方向相反，点乘为-1.因为在单位向量的情况下，结果由两个向量夹角cos的值决定。而对于多维向量，向量点乘就是对应位置乘积之后相加，得到的结果仍然是标量，含义和二维向量一样。</p>
<p>向量内积又可以体现两个向量之间的相似性</p>
<p>非对角线上的值代表第 $i$ 个特征和第 $j$ 个特征的相关性，比如<strong>共同出现</strong>，<strong>此消彼长</strong>等等</p>
<p>协方差的本质是内积。标准差的本质是模长。相关系数的本质是夹角余弦。</p>
<p>点积本身的性质，在二维空间，点积可以想象成一条直线在另一条直线上的投影。点击为零表示相互垂直。</p>
<p><strong>Gram 矩阵与协方差矩阵和相关系数矩阵差别</strong></p>
<p>Gram 矩阵和协方差矩阵的差别在于，Gram 矩阵<strong>没有白化</strong>，也就是没有<strong>减去均值</strong>，直接使用两向量做内积。</p>
<p>Gram 矩阵和相关系数矩阵的差别在于，Gram 矩阵既<strong>没有白化</strong>，也没有标准化（也就是除以两个向量的标准差）。</p>
<p>Gram 所表达的意义和协方差矩阵相差不大，只是显得比较粗糙。两个向量的协方差表示两向量之间的相似程度，协方差越大越相似。对角线的元素的值越大，表示其所代表的向量或者说特征越重要。</p>
<blockquote>
<p>A neural algorithm of artistic style. Gatys et al, 2015.</p>
</blockquote>
<hr>
<h4 id="4-11-一维到三维推广"><a href="#4-11-一维到三维推广" class="headerlink" title="4.11 一维到三维推广"></a>4.11 一维到三维推广</h4><p>我们之前介绍的CNN网络处理的都是2D图片，举例来介绍2D卷积的规则：</p>
<p><img src="assets/20180115210830278" alt="这里写图片描述"></p>
<p>输入图片维度：14 x 14 x 3</p>
<p>滤波器尺寸：5 x 5 x 3，滤波器个数：16</p>
<p>输出图片维度：10 x 10 x 16</p>
<p>将2D卷积推广到1D卷积，举例<strong>EKG</strong>心跳信号来介绍1D卷积的规则：</p>
<p><img src="assets/20180115210923590" alt="这里写图片描述"></p>
<p>输入时间序列维度：14 x 1</p>
<p>滤波器尺寸：5 x 1，滤波器个数：16</p>
<p>输出时间序列维度：10 x 16</p>
<p>对于3D卷积，举例来介绍其规则：</p>
<p><img src="assets/20180115211442581" alt="这里写图片描述"></p>
<p>输入3D图片维度：14 x 14 x 14 x 1</p>
<p>滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16</p>
<p>输出3D图片维度：10 x 10 x 10 x 16</p>
<hr>
<h1 id="深度学习工程师-1"><a href="#深度学习工程师-1" class="headerlink" title="深度学习工程师"></a>深度学习工程师</h1><p>由 deeplearning.ai 出品，网易引进的正版授权中文版深度学习工程师微专业课程，让你在了解丰富的人工智能应用案例的同时，学会在实践中搭建出最先进的神经网络模型，训练出属于你自己的 AI。</p>
<p>deeplearning.ai</p>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll">https://www.coursera.org/learn/neural-networks-deep-learning?action=enroll</a></p>
<p><a target="_blank" rel="noopener" href="https://study.163.com/my#/smarts">https://study.163.com/my#/smarts</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av66647398">https://www.bilibili.com/video/av66647398</a></p>
<p><strong>note</strong></p>
<p><a target="_blank" rel="noopener" href="https://redstonewill.blog.csdn.net/article/details/79446105">https://redstonewill.blog.csdn.net/article/details/79446105</a></p>
<p><a target="_blank" rel="noopener" href="http://www.ai-start.com/dl2017/">http://www.ai-start.com/dl2017/</a></p>
<p><strong>课后作业</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u013733326/article/details/79827273">https://blog.csdn.net/u013733326/article/details/79827273</a></p>
<p><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/5e20243e2823a10036b542da">https://www.heywhale.com/mw/project/5e20243e2823a10036b542da</a></p>
<h2 id="Question-1"><a href="#Question-1" class="headerlink" title="Question"></a>Question</h2><ul>
<li>[ ] 特殊应用：人脸识别和神经风格转换-<a href="#4.7">4.7 CNN可视化解释</a>，伪影</li>
</ul>
<h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><h3 id="第一周-循环序列模型"><a href="#第一周-循环序列模型" class="headerlink" title="第一周 循环序列模型"></a>第一周 循环序列模型</h3><h4 id="1-1-为什么选择序列模型"><a href="#1-1-为什么选择序列模型" class="headerlink" title="1.1 为什么选择序列模型"></a>1.1 为什么选择序列模型</h4><p><img src="assets/20180305154333482" alt="这里写图片描述"></p>
<ul>
<li><strong>语音识别</strong>：给定输入音频片段 X，输出一段文字 Y。输入输出都是序列模型，因为 X 是一个按时序播放的音频片段，Y 是一系列单词。</li>
<li><strong>音乐发生器</strong>：输入数据 X 可以是空集，也可以是单一的整数（表示音乐风格），也可能是生成的前几个音符。输出音乐序列 Y。</li>
<li><strong>情感分类</strong>：输入 X 是文字序列，输出评价指标。</li>
<li><strong>DNA序列分析</strong>：输入<strong>DNA</strong>序列 A、C、G、T 组成，输出各部分匹配的某种蛋白质。</li>
<li><strong>机器翻译</strong>：输入句子，输出另一种语言的翻译结果。</li>
<li><strong>视频动作识别</strong>：输入视频帧，输出识别其中的行为。</li>
<li><strong>命名实体识别</strong>：输入句子，暑促识别出句中的人名。</li>
</ul>
<p>这些序列模型基本都属于监督式学习，输入 X 和输出 Y 不一定都是序列模型。如果都是序列模型的话，模型长度不一定完全一致。</p>
<hr>
<h4 id="1-2-数学符号"><a href="#1-2-数学符号" class="headerlink" title="1.2 数学符号"></a>1.2 数学符号</h4><p>下面以命名实体识别为例，介绍序列模型的命名规则。示例语句为：</p>
<p><strong>Harry Potter and Hermione Granger invented a new spell.</strong></p>
<p>输入 x 包含 9 个单词，输出 y 即为 1 x 9 向量，每位表征对应单词是否为人名的一部分，1 表示是，0 表示否。很明显，该句话中“Harry”，“Potter”，“Hermione”，“Granger”均是人名成分，所以，对应的输出y可表示为：</p>
<script type="math/tex; mode=display">
y=[1\quad 1\quad  0\quad  1\quad  1\quad  0\quad  0\quad  0\quad  0]</script><p>一般约定使用 $y^{<t>}$ 表示序列对应位置的输出，使用 $T_y$ 表示输出序列长度，则 $1≤t≤T_y$。</p>
<p>对于输入 x，表示为：</p>
<script type="math/tex; mode=display">
[x^{<1>}\quad x^{<2>}\quad x^{<3>}\quad x^{<4>}\quad x^{<5>}\quad x^{<6>}\quad x^{<7>}\quad x^{<8>}\quad x^{<9>}]</script><p>同样， $x^{<t>}$ 表示序列对应位置的输入，$T_x$ 表示输入序列长度。注意，此例中，$T_x=T_y=9$，但是也存在 $T_x≠T_y$ 的情况。</p>
<p>如何来表示每个 $x^{<t>}$ 呢？方法是首先建立一个词汇库 vocabulary，尽可能包含更多的词汇。例如一个包含 10000 个词汇的词汇库为：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
a \\
and \\
\cdot \\
\cdot \\
\cdot \\
harry \\
\cdot \\
\cdot \\
\cdot \\
potter \\
\cdot \\
\cdot \\
\cdot \\
zulu
\end{matrix}
\right]</script><p>该词汇库可看成是 10000 x 1 的向量。值得注意的是自然语言处理 NLP 实际应用中的词汇库可达百万级别的词汇量。</p>
<p>然后，使用 one-hot 编码，例句中的每个单词 $x^{<t>}$ 都可以表示成10000 x 1的向量，词汇表中与 $x^{<t>}$ 对应的位置为 1，其它位置为 0。该 $x^{<t>}$ 为one-hot向量。值得一提的是如果出现词汇表之外的单词，可以使用 <strong>UNK</strong> （<strong>Unknow Word</strong>）或其他字符串来表示。</p>
<p>对于多样本，以上序列模型对应的命名规则可表示为：$x^{(i)<t>}$，$y^{(i)<t>}$，$T_x^{(i)}$，$T_y^{(i)}$。其中，$i$ 表示第 $i$ 个样本。不同样本的 $T_x^{(i)}$ 或 $T_y^{(i)}$ 都有可能不同。</p>
<hr>
<h4 id="1-3-循环神经网络模型"><a href="#1-3-循环神经网络模型" class="headerlink" title="1.3 循环神经网络模型"></a>1.3 循环神经网络模型</h4><p>对于序列模型，如果使用标准的神经网络，其模型结构如下：</p>
<p><img src="assets/20180305180556590" alt="这里写图片描述"></p>
<p>使用标准的神经网络模型存在两个问题：</p>
<p>第一个问题，<strong>不同样本的输入序列长度或输出序列长度不同</strong>，即 $T_x^{(i)}\neq T_x^{(j)}$，$T_y^{(i)}\neq T_y^{(j)}$，造成模型难以统一。解决办法之一是设定一个最大序列长度，对每个输入和输出序列<strong>补零</strong>并统一到最大长度。但是这种做法实际效果并不理想。</p>
<p>第二个问题，也是主要问题，这种标准神经网络结构并<strong>不共享</strong>从文本的不同位置上<strong>学到的特征</strong>。例如，如果 $x^{<1>}$ 是“Harry”是人名成分，我们希望当句子其它位置 $x^{<5>}$ 出现了 “Harry”，可以用到位置 1 已经学到的特征将它识别出来，这是<strong>共享特征</strong>的结果，如同 CNN 网络特点一样（将部分图片里学到的内容快速推广到图片的其他部分）。但是，上图所示的网络不具备共享特征的能力。值得一提的是，共享特征还有助于减少神经网络中的参数数量，一定程度上减小了模型的计算复杂度。例如上图所示的标准神经网络，假设每个 $x^{<t>}$ 扩展到最大序列长度为 100，且词汇表长度为 10000，则输入层就已经包含了 100 x 10000 个神经元了，权重参数很多，运算量将是庞大的。</p>
<p>标准的神经网络不适合解决序列模型问题，而循环神经网络（RNN）是专门用来解决序列模型问题的。RNN 模型结构如下：</p>
<p><img src="assets/20180305203908747" alt="这里写图片描述"></p>
<p>序列模型从左到右，依次传递，此例中， $T_x= T_y$。 $x^{<t>}$ 到 $y^{<t>}$ 之间是隐藏神经元。$a^{<t>}$ 会传入到第 $t+1$ 个元素中，作为输入。其中，$a^{<0>}$ 一般为零向量。</p>
<blockquote>
<p>如果从左到右的顺序读这个句子，将第一个词 $x^{<1>}$ 输入一个神经网络层，可以让神经网络尝试预测输出 $\hat y^{<1>}$，判断这是否是人名的一部分。当读到句中的第二个单词 $x^{<2>}$ 时，它不是仅用 $x^{<2>}$ 就预测出 $\hat y^{<2>}$，它也会输入一些来自时间步 1 的信息。具体而言，时间步 1 的激活值就会传递到时间步 2。然后，在下一个时间步，循环神经网络输入了单词 $x^{<3>}$ ，然后它尝试预测输出了预测结果 $\hat y^{<3>}$，等等，一直到最后一个时间步，输入 $x^{<T_x>}$ ，然后输出 $\hat y^{<T_y>}$。</p>
<p>如果 $T_x$ 和 $T_y$ 不相同，结构会需要作出一些改变。所以<strong>在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算</strong>。</p>
</blockquote>
<p>RNN模型包含三类权重系数，分别是 $W_{ax}$，$W_{aa}$，$W_{ya}$。且不同元素之间同一位置共享同一权重系数。$W_ax$ 来表示管理着从 $x^{<1>}$ 到隐藏层的连接的一系列参数，每个时间步使用的都是<strong>相同的参数</strong> $W_ax$。而激活值也就是水平联系是由参数 $W_{aa}$ 决定的，同时每一个时间步都使用<strong>相同的参数</strong> $W_{aa}$，同样的输出结果由 $W_{ya}$ 决定 。</p>
<p><img src="assets/20180305212325555" alt="这里写图片描述"></p>
<p>RNN的正向传播（Forward Propagation）过程为：</p>
<script type="math/tex; mode=display">
a^{<t>}=g(W_{aa}\cdot a^{<t-1>}+W_{ax}\cdot x^{<t>}+ba)\\
\hat y^{<t>}=g(W_{ya}\cdot a^{<t>}+b_y)</script><p>其中，$g(⋅)$ 表示激活函数，不同的问题需要使用不同的激活函数。</p>
<p>为了简化表达式，可以对 $a^{<t>}$ 项进行整合：</p>
<script type="math/tex; mode=display">
W_{aa}\cdot a^{<t-1>}+W_{ax}\cdot x^{<t>}=[W_{aa}\ \ W_{ax}]\left[
\begin{matrix}
a^{<t-1>} \\
x^{<t>}
\end{matrix}
\right]\rightarrow W_a[a^{<t-1>},x^{<t>}]</script><p>则正向传播可表示为：</p>
<script type="math/tex; mode=display">
a^{<t>}=g(W_a[a^{<t-1>},x^{<t>}]+b_a)\\
\hat y^{<t>}=g(W_{y}\cdot a^{<t>}+b_y)</script><p>值得一提的是，以上所述的RNN为单向RNN，即按照从左到右顺序，单向进行，$\hat y^{<t>}$ 只与左边的元素有关。但是，有时候 $\hat y^{<t>}$ 也可能与右边元素有关。例如下面两个句子中，单凭前三个单词，无法确定 “Teddy” 是否为人名，必须根据右边单词进行判断。</p>
<p>He said, “Teddy Roosevelt was a great President.”</p>
<p>He said, “Teddy bears are on sale!”</p>
<p>因此，有另外一种RNN结构是双向RNN，简称为BRNN。$\hat y^{<t>}$ 与左右元素均有关系，我们之后再详细介绍。</p>
<hr>
<h4 id="1-4-通过时间的反向传播"><a href="#1-4-通过时间的反向传播" class="headerlink" title="1.4 通过时间的反向传播"></a>1.4 通过时间的反向传播</h4><p></p>
<p></p>
<h4 id="1-5-不同类型的循环神经网络"><a href="#1-5-不同类型的循环神经网络" class="headerlink" title="1.5 不同类型的循环神经网络"></a>1.5 不同类型的循环神经网络</h4><p></p>
<p></p>
<h4 id="1-6-语言模型和序列生成"><a href="#1-6-语言模型和序列生成" class="headerlink" title="1.6 语言模型和序列生成"></a>1.6 语言模型和序列生成</h4><p></p>
<p></p>
<h4 id="1-7-对新序列采样"><a href="#1-7-对新序列采样" class="headerlink" title="1.7 对新序列采样"></a>1.7 对新序列采样</h4><p></p>
<p></p>
<h4 id="1-8-带有神经网络的梯度消失"><a href="#1-8-带有神经网络的梯度消失" class="headerlink" title="1.8 带有神经网络的梯度消失"></a>1.8 带有神经网络的梯度消失</h4><p></p>
<p></p>
<h4 id="1-9-GRU-单元"><a href="#1-9-GRU-单元" class="headerlink" title="1.9 GRU 单元"></a>1.9 GRU 单元</h4><p></p>
<p></p>
<h4 id="1-10-长短期记忆（LSTM）"><a href="#1-10-长短期记忆（LSTM）" class="headerlink" title="1.10 长短期记忆（LSTM）"></a>1.10 长短期记忆（LSTM）</h4><p></p>
<p></p>
<h4 id="1-11-双向神经网络"><a href="#1-11-双向神经网络" class="headerlink" title="1.11 双向神经网络"></a>1.11 双向神经网络</h4><p></p>
<p></p>
<h4 id="1-12-深层循环神经网络"><a href="#1-12-深层循环神经网络" class="headerlink" title="1.12 深层循环神经网络"></a>1.12 深层循环神经网络</h4><p></p>
<p></p>
<h3 id="第二周-自然语言处理与词嵌入"><a href="#第二周-自然语言处理与词嵌入" class="headerlink" title="第二周 自然语言处理与词嵌入"></a>第二周 自然语言处理与词嵌入</h3><h4 id="2-1-词汇表征"><a href="#2-1-词汇表征" class="headerlink" title="2.1 词汇表征"></a>2.1 词汇表征</h4><p></p>
<h4 id="2-2-使用词嵌入"><a href="#2-2-使用词嵌入" class="headerlink" title="2.2 使用词嵌入"></a>2.2 使用词嵌入</h4><p></p>
<h4 id="2-3-词嵌入的特性"><a href="#2-3-词嵌入的特性" class="headerlink" title="2.3 词嵌入的特性"></a>2.3 词嵌入的特性</h4><p></p>
<h4 id="2-4-嵌入矩阵"><a href="#2-4-嵌入矩阵" class="headerlink" title="2.4 嵌入矩阵"></a>2.4 嵌入矩阵</h4><p></p>
<h4 id="2-5-学习词嵌入"><a href="#2-5-学习词嵌入" class="headerlink" title="2.5 学习词嵌入"></a>2.5 学习词嵌入</h4><p></p>
<h4 id="2-6-Word2Vec"><a href="#2-6-Word2Vec" class="headerlink" title="2.6 Word2Vec"></a>2.6 Word2Vec</h4><p></p>
<h4 id="2-7-负采样"><a href="#2-7-负采样" class="headerlink" title="2.7 负采样"></a>2.7 负采样</h4><p></p>
<h4 id="2-8-GloVe-词向量"><a href="#2-8-GloVe-词向量" class="headerlink" title="2.8 GloVe 词向量"></a>2.8 GloVe 词向量</h4><p></p>
<h4 id="2-9-情绪分类"><a href="#2-9-情绪分类" class="headerlink" title="2.9 情绪分类"></a>2.9 情绪分类</h4><p></p>
<h4 id="2-10-词嵌入除偏"><a href="#2-10-词嵌入除偏" class="headerlink" title="2.10 词嵌入除偏"></a>2.10 词嵌入除偏</h4><p></p>
<p></p>
<h3 id="第三周-序列模型和注意力机制"><a href="#第三周-序列模型和注意力机制" class="headerlink" title="第三周 序列模型和注意力机制"></a>第三周 序列模型和注意力机制</h3><h4 id="3-1-基础模型"><a href="#3-1-基础模型" class="headerlink" title="3.1 基础模型"></a>3.1 基础模型</h4><p></p>
<p></p>
<h4 id="3-2-选择最可能的句子"><a href="#3-2-选择最可能的句子" class="headerlink" title="3.2 选择最可能的句子"></a>3.2 选择最可能的句子</h4><p></p>
<p></p>
<h4 id="3-3-定向搜索"><a href="#3-3-定向搜索" class="headerlink" title="3.3 定向搜索"></a>3.3 定向搜索</h4><p></p>
<p></p>
<h4 id="3-4-改进定向搜索"><a href="#3-4-改进定向搜索" class="headerlink" title="3.4 改进定向搜索"></a>3.4 改进定向搜索</h4><p></p>
<p></p>
<h4 id="3-5-定向搜索的误差分析"><a href="#3-5-定向搜索的误差分析" class="headerlink" title="3.5 定向搜索的误差分析"></a>3.5 定向搜索的误差分析</h4><p></p>
<p></p>
<h4 id="3-6-Bleu-得分（选修）"><a href="#3-6-Bleu-得分（选修）" class="headerlink" title="3.6 Bleu 得分（选修）"></a>3.6 Bleu 得分（选修）</h4><p></p>
<p></p>
<h4 id="3-7-注意力模型直观理解"><a href="#3-7-注意力模型直观理解" class="headerlink" title="3.7 注意力模型直观理解"></a>3.7 注意力模型直观理解</h4><p></p>
<p></p>
<h4 id="3-8-注意力模型"><a href="#3-8-注意力模型" class="headerlink" title="3.8 注意力模型"></a>3.8 注意力模型</h4><p></p>
<p></p>
<h4 id="3-9-语音辨识"><a href="#3-9-语音辨识" class="headerlink" title="3.9 语音辨识"></a>3.9 语音辨识</h4><p></p>
<p></p>
<h4 id="3-10-触发字检测"><a href="#3-10-触发字检测" class="headerlink" title="3.10 触发字检测"></a>3.10 触发字检测</h4><p></p>
<p></p>
<h4 id="3-11-结论和致谢"><a href="#3-11-结论和致谢" class="headerlink" title="3.11 结论和致谢"></a>3.11 结论和致谢</h4><p></p>
<p></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/03/28/C-DL-W3/" rel="next" title="C_DL_W3">
                <i class="fa fa-chevron-left"></i> C_DL_W3
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2022/03/28/C-DL-W5/" rel="prev" title="C_DL_W5">
                C_DL_W5 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88"><span class="nav-number">1.</span> <span class="nav-text">深度学习工程师</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Question"><span class="nav-number">1.1.</span> <span class="nav-text">Question</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.1.</span> <span class="nav-text">第一周 卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1.1 计算机视觉</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">1.2 边缘检测示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E6%9B%B4%E5%A4%9A%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E5%86%85%E5%AE%B9"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">1.3 更多边缘检测内容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-Padding"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">1.4 Padding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-%E5%8D%B7%E7%A7%AF%E6%AD%A5%E9%95%BF"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">1.5 卷积步长</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-%E4%B8%89%E7%BB%B4%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.2.1.6.</span> <span class="nav-text">1.6 三维卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-%E5%8D%95%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.1.7.</span> <span class="nav-text">1.7 单层卷积网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-8-%E7%AE%80%E5%8D%95%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.1.8.</span> <span class="nav-text">1.8 简单卷积网络示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-9-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">1.2.1.9.</span> <span class="nav-text">1.9 池化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-10-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.2.1.10.</span> <span class="nav-text">1.10 卷积神经网络示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-11-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%EF%BC%9F"><span class="nav-number">1.2.1.11.</span> <span class="nav-text">1.11 为什么使用卷积？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-12-interview-yann-lecun"><span class="nav-number">1.2.1.12.</span> <span class="nav-text">1.12 interview yann-lecun</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%9A%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6"><span class="nav-number">1.2.2.</span> <span class="nav-text">第二周 深度卷积网络：实例探究</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E5%AE%9E%E4%BE%8B%E6%8E%A2%E7%A9%B6%EF%BC%9F"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.1 为什么要进行实例探究？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.2 经典网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">2.3 残差网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8%EF%BC%9F"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">2.4 残差网络为什么有用？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-Network-in-network-%E4%BB%A5%E5%8F%8A-1%C3%971-%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">2.5 Network in network 以及 1×1 卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-Inception-network-motivation"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">2.6 Inception network motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-Inception-%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">2.7 Inception 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-8-MobileNet"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">2.8 MobileNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-9-MobileNet-%E6%9E%B6%E6%9E%84"><span class="nav-number">1.2.2.9.</span> <span class="nav-text">2.9 MobileNet 架构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-10-EfficientNet"><span class="nav-number">1.2.2.10.</span> <span class="nav-text">2.10 EfficientNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-11-%E4%BD%BF%E7%94%A8%E5%BC%80%E6%BA%90%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88"><span class="nav-number">1.2.2.11.</span> <span class="nav-text">2.11 使用开源的实现方案</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-12-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.2.12.</span> <span class="nav-text">2.12 迁移学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-13-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="nav-number">1.2.2.13.</span> <span class="nav-text">2.13 数据增强</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-14-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%8E%B0%E7%8A%B6"><span class="nav-number">1.2.2.14.</span> <span class="nav-text">2.14 计算机视觉现状</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-number">1.2.3.</span> <span class="nav-text">第三周 目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E7%9B%AE%E6%A0%87%E5%AE%9A%E4%BD%8D"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">3.1 目标定位</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E7%89%B9%E5%BE%81%E7%82%B9%E6%A3%80%E6%B5%8B"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">3.2 特征点检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">3.3 目标检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">3.4 卷积的滑动窗口实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-Bounding-Box-%E9%A2%84%E6%B5%8B"><span class="nav-number">1.2.3.5.</span> <span class="nav-text">3.5 Bounding Box 预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-%E4%BA%A4%E5%B9%B6%E6%AF%94"><span class="nav-number">1.2.3.6.</span> <span class="nav-text">3.6 交并比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-7-%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6"><span class="nav-number">1.2.3.7.</span> <span class="nav-text">3.7 非极大值抑制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-Anchor-Boxes"><span class="nav-number">1.2.3.8.</span> <span class="nav-text">3.8 Anchor Boxes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-9-YOLO-%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.3.9.</span> <span class="nav-text">3.9 YOLO 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-10-R-CNN"><span class="nav-number">1.2.3.10.</span> <span class="nav-text">3.10 R-CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2U-Net"><span class="nav-number">1.2.3.11.</span> <span class="nav-text">3.11 语义分割U-Net</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-12-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.2.3.12.</span> <span class="nav-text">3.12 转置卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-13-U-Net%E6%9E%B6%E6%9E%84%E7%9B%B4%E8%A7%89"><span class="nav-number">1.2.3.13.</span> <span class="nav-text">3.13 U-Net架构直觉</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-14-U-Net%E6%9E%B6%E6%9E%84"><span class="nav-number">1.2.3.14.</span> <span class="nav-text">3.14 U-Net架构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E5%91%A8-%E7%89%B9%E6%AE%8A%E5%BA%94%E7%94%A8%EF%BC%9A%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%92%8C%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2"><span class="nav-number">1.2.4.</span> <span class="nav-text">第四周 特殊应用：人脸识别和神经风格转换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%EF%BC%9F"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">4.1 什么是人脸识别？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-One-Shot-%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">4.2 One-Shot 学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Siamese-%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">4.3 Siamese 网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-Triplet-%E6%8D%9F%E5%A4%B1"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">4.4 Triplet 损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-%E9%9D%A2%E9%83%A8%E9%AA%8C%E8%AF%81%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="nav-number">1.2.4.5.</span> <span class="nav-text">4.5 面部验证与二分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-6-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E6%8D%A2%EF%BC%9F"><span class="nav-number">1.2.4.6.</span> <span class="nav-text">4.6 什么是神经风格转换？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-7-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%AD%A6%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.2.4.7.</span> <span class="nav-text">4.7 深度卷积网络在学什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-8-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.4.8.</span> <span class="nav-text">4.8 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-9-content-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.4.9.</span> <span class="nav-text">4.9 content 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-10-style-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.4.10.</span> <span class="nav-text">4.10 style 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-11-%E4%B8%80%E7%BB%B4%E5%88%B0%E4%B8%89%E7%BB%B4%E6%8E%A8%E5%B9%BF"><span class="nav-number">1.2.4.11.</span> <span class="nav-text">4.11 一维到三维推广</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88-1"><span class="nav-number">2.</span> <span class="nav-text">深度学习工程师</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Question-1"><span class="nav-number">2.1.</span> <span class="nav-text">Question</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8-%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.1.</span> <span class="nav-text">第一周 循环序列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">1.1 为什么选择序列模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">1.2 数学符号</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.1.3.</span> <span class="nav-text">1.3 循环神经网络模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.2.1.4.</span> <span class="nav-text">1.4 通过时间的反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.1.5.</span> <span class="nav-text">1.5 不同类型的循环神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-6-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90"><span class="nav-number">2.2.1.6.</span> <span class="nav-text">1.6 语言模型和序列生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-%E5%AF%B9%E6%96%B0%E5%BA%8F%E5%88%97%E9%87%87%E6%A0%B7"><span class="nav-number">2.2.1.7.</span> <span class="nav-text">1.7 对新序列采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-8-%E5%B8%A6%E6%9C%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">2.2.1.8.</span> <span class="nav-text">1.8 带有神经网络的梯度消失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-9-GRU-%E5%8D%95%E5%85%83"><span class="nav-number">2.2.1.9.</span> <span class="nav-text">1.9 GRU 单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-10-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%EF%BC%88LSTM%EF%BC%89"><span class="nav-number">2.2.1.10.</span> <span class="nav-text">1.10 长短期记忆（LSTM）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-11-%E5%8F%8C%E5%90%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.1.11.</span> <span class="nav-text">1.11 双向神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-12-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.1.12.</span> <span class="nav-text">1.12 深层循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">2.2.2.</span> <span class="nav-text">第二周 自然语言处理与词嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%BE%81"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">2.1 词汇表征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E4%BD%BF%E7%94%A8%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2.2 使用词嵌入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-%E8%AF%8D%E5%B5%8C%E5%85%A5%E7%9A%84%E7%89%B9%E6%80%A7"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">2.3 词嵌入的特性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-%E5%B5%8C%E5%85%A5%E7%9F%A9%E9%98%B5"><span class="nav-number">2.2.2.4.</span> <span class="nav-text">2.4 嵌入矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-%E5%AD%A6%E4%B9%A0%E8%AF%8D%E5%B5%8C%E5%85%A5"><span class="nav-number">2.2.2.5.</span> <span class="nav-text">2.5 学习词嵌入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-Word2Vec"><span class="nav-number">2.2.2.6.</span> <span class="nav-text">2.6 Word2Vec</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="nav-number">2.2.2.7.</span> <span class="nav-text">2.7 负采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-8-GloVe-%E8%AF%8D%E5%90%91%E9%87%8F"><span class="nav-number">2.2.2.8.</span> <span class="nav-text">2.8 GloVe 词向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-9-%E6%83%85%E7%BB%AA%E5%88%86%E7%B1%BB"><span class="nav-number">2.2.2.9.</span> <span class="nav-text">2.9 情绪分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-10-%E8%AF%8D%E5%B5%8C%E5%85%A5%E9%99%A4%E5%81%8F"><span class="nav-number">2.2.2.10.</span> <span class="nav-text">2.10 词嵌入除偏</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">2.2.3.</span> <span class="nav-text">第三周 序列模型和注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">3.1 基础模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E9%80%89%E6%8B%A9%E6%9C%80%E5%8F%AF%E8%83%BD%E7%9A%84%E5%8F%A5%E5%AD%90"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">3.2 选择最可能的句子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E5%AE%9A%E5%90%91%E6%90%9C%E7%B4%A2"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">3.3 定向搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-%E6%94%B9%E8%BF%9B%E5%AE%9A%E5%90%91%E6%90%9C%E7%B4%A2"><span class="nav-number">2.2.3.4.</span> <span class="nav-text">3.4 改进定向搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-%E5%AE%9A%E5%90%91%E6%90%9C%E7%B4%A2%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="nav-number">2.2.3.5.</span> <span class="nav-text">3.5 定向搜索的误差分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-6-Bleu-%E5%BE%97%E5%88%86%EF%BC%88%E9%80%89%E4%BF%AE%EF%BC%89"><span class="nav-number">2.2.3.6.</span> <span class="nav-text">3.6 Bleu 得分（选修）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-7-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-number">2.2.3.7.</span> <span class="nav-text">3.7 注意力模型直观理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-8-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.3.8.</span> <span class="nav-text">3.8 注意力模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-9-%E8%AF%AD%E9%9F%B3%E8%BE%A8%E8%AF%86"><span class="nav-number">2.2.3.9.</span> <span class="nav-text">3.9 语音辨识</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-10-%E8%A7%A6%E5%8F%91%E5%AD%97%E6%A3%80%E6%B5%8B"><span class="nav-number">2.2.3.10.</span> <span class="nav-text">3.10 触发字检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-11-%E7%BB%93%E8%AE%BA%E5%92%8C%E8%87%B4%E8%B0%A2"><span class="nav-number">2.2.3.11.</span> <span class="nav-text">3.11 结论和致谢</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Duan Yaqi</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>


<div class="powered-by">
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <i class="fa fa-user-md"></i>
    <span id="busuanzi_container_site_uv">
        本站访客数:<span id="busuanzi_value_site_uv"></span>
    </span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_pv">    
        本站访问量<span id="busuanzi_value_site_pv"></span>
    </span>
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
